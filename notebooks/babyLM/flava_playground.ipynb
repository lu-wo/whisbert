{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flava standard model without heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n",
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'mlm_head.transform.dense.weight', 'mmm_text_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'mmm_image_head.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'itm_head.seq_relationship.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'mmm_image_head.decoder.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'mlm_head.transform.dense.bias', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'mmm_text_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'mmm_text_head.decoder.weight', 'mmm_text_head.transform.dense.weight', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'mlm_head.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'mmm_image_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'mim_head.decoder.bias', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'mim_head.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'itm_head.pooler.dense.bias', 'mmm_text_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.input.bias', 'mmm_text_head.bias', 'mlm_head.transform.LayerNorm.bias', 'mmm_image_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'mlm_head.decoder.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'mim_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.output.conv.bias', 'mmm_text_head.transform.dense.bias', 'itm_head.seq_relationship.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.input.weight', 'mmm_image_head.transform.dense.weight', 'mim_head.transform.dense.bias', 'mim_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight']\n",
      "- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/modeling_utils.py:862: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/feature_extraction_flava.py:28: FutureWarning: The class FlavaFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use FlavaImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import FlavaProcessor, FlavaModel\n",
    "\n",
    "model = FlavaModel.from_pretrained(\"facebook/flava-full\")\n",
    "processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=[image, image],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=77,\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "image_embeddings = (\n",
    "    outputs.image_embeddings\n",
    ")  # Batch size X (Number of image patches + 1) x Hidden size => 2 X 197 X 768\n",
    "text_embeddings = (\n",
    "    outputs.text_embeddings\n",
    ")  # Batch size X (Text sequence length + 1) X Hidden size => 2 X 77 X 768\n",
    "multimodal_embeddings = (\n",
    "    outputs.multimodal_embeddings\n",
    ")  # Batch size X (Number of image patches + Text Sequence Length + 3) X Hidden size => 2 X 275 x 768\n",
    "# Multimodal embeddings can be used for multimodal tasks such as VQA\n",
    "\n",
    "\n",
    "## Pass only image\n",
    "from transformers import FlavaFeatureExtractor\n",
    "\n",
    "feature_extractor = FlavaFeatureExtractor.from_pretrained(\"facebook/flava-full\")\n",
    "inputs = feature_extractor(images=[image, image], return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "image_embeddings = outputs.image_embeddings\n",
    "\n",
    "## Pass only image\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"facebook/flava-full\")\n",
    "inputs = tokenizer(\n",
    "    [\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=77,\n",
    ")\n",
    "outputs = model(**inputs)\n",
    "text_embeddings = outputs.text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlavaModel(\n",
       "  (text_model): FlavaTextModel(\n",
       "    (embeddings): FlavaTextEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): FlavaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x FlavaLayer(\n",
       "          (attention): FlavaAttention(\n",
       "            (attention): FlavaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): FlavaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): FlavaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): FlavaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): FlavaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (image_model): FlavaImageModel(\n",
       "    (embeddings): FlavaImageEmbeddings(\n",
       "      (patch_embeddings): PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): FlavaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x FlavaLayer(\n",
       "          (attention): FlavaAttention(\n",
       "            (attention): FlavaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): FlavaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): FlavaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): FlavaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): FlavaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (multimodal_model): FlavaMultimodalModel(\n",
       "    (encoder): FlavaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x FlavaLayer(\n",
       "          (attention): FlavaAttention(\n",
       "            (attention): FlavaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): FlavaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): FlavaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): FlavaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): FlavaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (image_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (image_to_mm_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (text_to_mm_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flava pretraining with heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n",
      "`input_ids_masked` isn't passed which means MLM loss won't be calculated correctlySetting it to `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if you are doing inference on unmasked text...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 480)\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'codebook_pixel_values', 'bool_masked_pos'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/modeling_utils.py:862: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import FlavaProcessor, FlavaForPreTraining\n",
    "\n",
    "model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\")\n",
    "processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "print(image.size)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=[image, image],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=77,\n",
    "    return_codebook_pixels=True,\n",
    "    return_image_mask=True,\n",
    "    # Other things such as mlm_labels, itm_labels can be passed here. See docs\n",
    ")\n",
    "inputs.bool_masked_pos.zero_()\n",
    "\n",
    "print(inputs.keys())\n",
    "\n",
    "outputs = model(**inputs)\n",
    "image_embeddings = (\n",
    "    outputs.image_embeddings\n",
    ")  # Batch size X (Number of image patches + 1) x Hidden size => 2 X 197 X 768\n",
    "text_embeddings = (\n",
    "    outputs.text_embeddings\n",
    ")  # Batch size X (Text sequence length + 1) X Hidden size => 2 X 77 X 768\n",
    "# Multimodal embeddings can be used for multimodal tasks such as VQA\n",
    "multimodal_embeddings = (\n",
    "    outputs.multimodal_embeddings\n",
    ")  # Batch size X (Number of image patches + Text Sequence Length + 3) X Hidden size => 2 X 275 x 768\n",
    "\n",
    "# Loss\n",
    "loss = outputs.loss  # probably NaN due to missing labels\n",
    "\n",
    "# Global contrastive loss logits\n",
    "image_contrastive_logits = outputs.contrastive_logits_per_image\n",
    "text_contrastive_logits = outputs.contrastive_logits_per_text\n",
    "\n",
    "# ITM logits\n",
    "itm_logits = outputs.itm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'codebook_pixel_values', 'bool_masked_pos'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import FlavaImageProcessor, FlavaProcessor\n",
    "\n",
    "\n",
    "processor = FlavaImageProcessor(do_resize=False)\n",
    "# processor = FlavaProcessor(image_processor=processor)\n",
    "proc_image = processor(\n",
    "    images=torch.rand(3, 128, 1000), return_codebook_pixels=True, return_image_mask=True\n",
    ")\n",
    "proc_image.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_image.pixel_values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlavaForPreTraining(\n",
       "  (flava): FlavaModel(\n",
       "    (text_model): FlavaTextModel(\n",
       "      (embeddings): FlavaTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): FlavaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x FlavaLayer(\n",
       "            (attention): FlavaAttention(\n",
       "              (attention): FlavaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): FlavaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): FlavaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): FlavaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): FlavaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (image_model): FlavaImageModel(\n",
       "      (embeddings): FlavaImageEmbeddings(\n",
       "        (patch_embeddings): PatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): FlavaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x FlavaLayer(\n",
       "            (attention): FlavaAttention(\n",
       "              (attention): FlavaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): FlavaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): FlavaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): FlavaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): FlavaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (multimodal_model): FlavaMultimodalModel(\n",
       "      (encoder): FlavaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x FlavaLayer(\n",
       "            (attention): FlavaAttention(\n",
       "              (attention): FlavaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): FlavaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): FlavaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): FlavaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): FlavaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (image_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (text_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (image_to_mm_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (text_to_mm_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (image_codebook): FlavaImageCodebook(\n",
       "    (blocks): Sequential(\n",
       "      (input): Conv2d(3, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (group_1): FlavaImageCodebookLayerGroup(\n",
       "        (group): Sequential(\n",
       "          (block_1): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block_2): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (group_2): FlavaImageCodebookLayerGroup(\n",
       "        (group): Sequential(\n",
       "          (block_1): FlavaImageCodebookBlock(\n",
       "            (id_path): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block_2): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (group_3): FlavaImageCodebookLayerGroup(\n",
       "        (group): Sequential(\n",
       "          (block_1): FlavaImageCodebookBlock(\n",
       "            (id_path): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block_2): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (group_4): FlavaImageCodebookLayerGroup(\n",
       "        (group): Sequential(\n",
       "          (block_1): FlavaImageCodebookBlock(\n",
       "            (id_path): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block_2): FlavaImageCodebookBlock(\n",
       "            (id_path): Identity()\n",
       "            (res_path): FlavaImageCodebookResPath(\n",
       "              (path): Sequential(\n",
       "                (relu_1): ReLU()\n",
       "                (conv_1): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_2): ReLU()\n",
       "                (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_3): ReLU()\n",
       "                (conv_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (relu_4): ReLU()\n",
       "                (conv_4): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output): Sequential(\n",
       "        (relu): ReLU()\n",
       "        (conv): Conv2d(2048, 8192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mim_head): FlavaMaskedPredictionHead(\n",
       "    (transform): FlavaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=8192, bias=True)\n",
       "  )\n",
       "  (mlm_head): FlavaMaskedPredictionHead(\n",
       "    (transform): FlavaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       "  (itm_head): FlavaITMHead(\n",
       "    (pooler): FlavaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (mmm_image_head): FlavaMaskedPredictionHead(\n",
       "    (transform): FlavaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=8192, bias=True)\n",
       "  )\n",
       "  (mmm_text_head): FlavaMaskedPredictionHead(\n",
       "    (transform): FlavaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       "  (global_contrastive_head): FlavaGlobalContrastiveHead()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'codebook_pixel_values', 'bool_masked_pos'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 112, 112])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.codebook_pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=[image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.2807141 ,  0.38290307,  0.42669836, ..., -0.28862458,\n",
       "          -0.2740262 , -0.28862458],\n",
       "         [ 0.32450938,  0.38290307,  0.41209993, ..., -0.28862458,\n",
       "          -0.28862458, -0.31782144],\n",
       "         [ 0.2807141 ,  0.3537062 ,  0.36830464, ..., -0.37621516,\n",
       "          -0.3470183 , -0.31782144],\n",
       "         ...,\n",
       "         [ 1.6383677 ,  1.5361787 ,  1.4193913 , ...,  1.3901944 ,\n",
       "           1.2880055 ,  1.2442101 ],\n",
       "         [ 1.6091708 ,  1.5507771 ,  1.5069818 , ...,  1.2150133 ,\n",
       "           0.9814385 ,  0.85005265],\n",
       "         [ 1.6091708 ,  1.477785  ,  1.4923834 , ...,  0.12013142,\n",
       "          -0.12804192, -0.39081356]],\n",
       " \n",
       "        [[-1.3919107 , -1.3919107 , -1.3919107 , ..., -1.5419884 ,\n",
       "          -1.5419884 , -1.5569961 ],\n",
       "         [-1.3468874 , -1.3468874 , -1.3468874 , ..., -1.5269806 ,\n",
       "          -1.5119728 , -1.5269806 ],\n",
       "         [-1.4069184 , -1.3769029 , -1.3468874 , ..., -1.5569961 ,\n",
       "          -1.5419884 , -1.5419884 ],\n",
       "         ...,\n",
       "         [-0.3413669 , -0.461429  , -0.55147564, ..., -0.6415222 ,\n",
       "          -0.7015533 , -0.7465766 ],\n",
       "         [-0.3413669 , -0.38639018, -0.49144456, ..., -0.7315688 ,\n",
       "          -0.8666388 , -0.92666984],\n",
       "         [-0.37138242, -0.49144456, -0.5064523 , ..., -1.2868563 ,\n",
       "          -1.3769029 , -1.4819573 ]],\n",
       " \n",
       "        [[-0.65545595, -0.49903518, -0.54169536, ..., -1.0109575 ,\n",
       "          -0.9256371 , -0.95407724],\n",
       "         [-0.6981161 , -0.5985757 , -0.57013553, ..., -1.0109575 ,\n",
       "          -0.95407724, -1.0109575 ],\n",
       "         [-0.61279577, -0.5274753 , -0.49903518, ..., -1.0251776 ,\n",
       "          -1.0393977 , -1.0536178 ],\n",
       "         ...,\n",
       "         [ 1.3637935 ,  1.3495734 ,  1.1220524 , ...,  1.1647125 ,\n",
       "           1.0651721 ,  0.9514116 ],\n",
       "         [ 1.3353534 ,  1.1789327 ,  1.264253  , ...,  0.93719155,\n",
       "           0.75233066,  0.6243501 ],\n",
       "         [ 1.3780135 ,  1.3780135 ,  1.264253  , ..., -0.12931348,\n",
       "          -0.5559154 , -0.74077636]]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAVA Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlavaProcessor, FlavaForPreTraining\n",
    "\n",
    "model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\")\n",
    "processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flava Image Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: cats-image/image\n",
      "Found cached dataset cats-image (/Users/lukas/.cache/huggingface/datasets/huggingface___cats-image/image/1.9.0/68fbc793fb10cd165e490867f5d61fa366086ea40c73e549a020103dcb4f597e)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1187.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    FlavaImageModel,\n",
    "    FlavaImageConfig,\n",
    "    FlavaConfig,\n",
    "    FlavaImageProcessor,\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "print(f\"Image shape {np.array(image).shape}\")\n",
    "\n",
    "image_processor = FlavaImageProcessor(\"facebook/flava-full\")\n",
    "image_config = FlavaImageConfig(num_channels=1)\n",
    "model = FlavaImageModel(image_config)\n",
    "\n",
    "flava_pre_config = FlavaConfig(image_config={\"num_channels\": 1})\n",
    "flava_pretrained = FlavaForPreTraining(flava_pre_config)\n",
    "\n",
    "inputs = image_processor(\n",
    "    image,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "# list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(1, 1, 224, 224)\n",
    "model(img).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`codebook_pixel_value` are required to generate `mim_labels` if loss is expected. Call `AutoProcessor` with `return_codebook_pixels` set to True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m flava_pretrained(pixel_values\u001b[39m=\u001b[39;49mimg)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:1890\u001b[0m, in \u001b[0;36mFlavaForPreTraining.forward\u001b[0;34m(self, input_ids, input_ids_masked, pixel_values, codebook_pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_unmasked_multimodal_encoder, mlm_labels, mim_labels, itm_labels, output_attentions, output_hidden_states, return_dict, return_loss)\u001b[0m\n\u001b[1;32m   1884\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1885\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`return_loss` is set to True but the image codebook is not initialized and no `mim_labels` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1886\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m have been passed. Reinstantiate the model with `init_codebook` set to True or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1887\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpass in your custom `mim_labels`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1888\u001b[0m             )\n\u001b[1;32m   1889\u001b[0m         \u001b[39mif\u001b[39;00m codebook_pixel_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1890\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1891\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`codebook_pixel_value` are required to generate `mim_labels` if loss is expected. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1892\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCall `AutoProcessor` with `return_codebook_pixels` set to True\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1893\u001b[0m             )\n\u001b[1;32m   1894\u001b[0m         mim_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_codebook\u001b[39m.\u001b[39mget_codebook_indices(codebook_pixel_values)\n\u001b[1;32m   1895\u001b[0m \u001b[39m# Unimodal MIM Loss\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m \u001b[39m# If multimodal embeddings are present, we will calculate MMM loss\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: `codebook_pixel_value` are required to generate `mim_labels` if loss is expected. Call `AutoProcessor` with `return_codebook_pixels` set to True"
     ]
    }
   ],
   "source": [
    "flava_pretrained(pixel_values=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m \n\u001b[1;32m      3\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model(pixel_values\u001b[39m=\u001b[39;49mimg)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:1840\u001b[0m, in \u001b[0;36mFlavaForPreTraining.forward\u001b[0;34m(self, input_ids, input_ids_masked, pixel_values, codebook_pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_unmasked_multimodal_encoder, mlm_labels, mim_labels, itm_labels, output_attentions, output_hidden_states, return_dict, return_loss)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1834\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`input_ids_masked` isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt passed which means MLM loss won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be calculated correctlySetting it to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1835\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1836\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m you are doing inference on unmasked text...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1837\u001b[0m     )\n\u001b[1;32m   1838\u001b[0m     input_ids_masked \u001b[39m=\u001b[39m input_ids\n\u001b[0;32m-> 1840\u001b[0m flava_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflava(\n\u001b[1;32m   1841\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1842\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1843\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1844\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1845\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1846\u001b[0m     image_attention_mask\u001b[39m=\u001b[39;49mimage_attention_mask,\n\u001b[1;32m   1847\u001b[0m     \u001b[39m# Don't need unmasked multimodal embedding for anything so skip it\u001b[39;49;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[39m# NOTE: ITM uses masked version\u001b[39;49;00m\n\u001b[1;32m   1849\u001b[0m     skip_multimodal_encoder\u001b[39m=\u001b[39;49mskip_unmasked_multimodal_encoder,\n\u001b[1;32m   1850\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1851\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1852\u001b[0m     \u001b[39m# Pass true to have deterministic outputs\u001b[39;49;00m\n\u001b[1;32m   1853\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1854\u001b[0m )\n\u001b[1;32m   1856\u001b[0m flava_masked_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflava(\n\u001b[1;32m   1857\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids_masked,\n\u001b[1;32m   1858\u001b[0m     pixel_values\u001b[39m=\u001b[39mpixel_values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1865\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1866\u001b[0m )\n\u001b[1;32m   1868\u001b[0m pos_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:1385\u001b[0m, in \u001b[0;36mFlavaModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_multimodal_encoder, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1383\u001b[0m image_output \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m \u001b[39mif\u001b[39;00m pixel_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1385\u001b[0m     image_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_model(\n\u001b[1;32m   1386\u001b[0m         pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1387\u001b[0m         bool_masked_pos\u001b[39m=\u001b[39;49mbool_masked_pos,\n\u001b[1;32m   1388\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mimage_attention_mask,\n\u001b[1;32m   1389\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1390\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1391\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1392\u001b[0m     )\n\u001b[1;32m   1393\u001b[0m     image_embeddings, image_states \u001b[39m=\u001b[39m image_output[\u001b[39m0\u001b[39m], image_output[\u001b[39m2\u001b[39m]\n\u001b[1;32m   1394\u001b[0m     \u001b[39m# Note that these states don't use final layernorm in the transformer model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:957\u001b[0m, in \u001b[0;36mFlavaImageModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    955\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 957\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    958\u001b[0m     pixel_values, bool_masked_pos\u001b[39m=\u001b[39;49mbool_masked_pos, interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding\n\u001b[1;32m    959\u001b[0m )\n\u001b[1;32m    961\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    962\u001b[0m     embedding_output,\n\u001b[1;32m    963\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    968\u001b[0m )\n\u001b[1;32m    969\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:309\u001b[0m, in \u001b[0;36mFlavaImageEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    305\u001b[0m     pixel_values: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    306\u001b[0m     bool_masked_pos: Optional[torch\u001b[39m.\u001b[39mBoolTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m     interpolate_pos_encoding: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 309\u001b[0m     batch_size, num_channels, height, width \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape\n\u001b[1;32m    310\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[39m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m    312\u001b[0m     batch_size, seq_len, _ \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39msize()\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "img = torch.rand(3, 224, 224)\n",
    "\n",
    "model(pixel_values=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean must have 1 elements if it is an iterable, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m \n\u001b[1;32m      4\u001b[0m audio_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m80\u001b[39m, \u001b[39m200\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m out \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39;49maudio_image)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/processing_flava.py:113\u001b[0m, in \u001b[0;36mFlavaProcessor.__call__\u001b[0;34m(self, images, text, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_image_mask, return_codebook_pixels, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m     95\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m     96\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(\n\u001b[1;32m    114\u001b[0m         images,\n\u001b[1;32m    115\u001b[0m         return_image_mask\u001b[39m=\u001b[39;49mreturn_image_mask,\n\u001b[1;32m    116\u001b[0m         return_codebook_pixels\u001b[39m=\u001b[39;49mreturn_codebook_pixels,\n\u001b[1;32m    117\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    118\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     encoding\u001b[39m.\u001b[39mupdate(image_features)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/image_processing_utils.py:458\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    457\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/image_processing_flava.py:664\u001b[0m, in \u001b[0;36mFlavaImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_image_mask, input_size_patches, total_mask_patches, mask_group_min_patches, mask_group_max_patches, mask_group_min_aspect_ratio, mask_group_max_aspect_ratio, return_codebook_pixels, codebook_do_resize, codebook_size, codebook_resample, codebook_do_center_crop, codebook_crop_size, codebook_do_rescale, codebook_rescale_factor, codebook_do_map_pixels, codebook_do_normalize, codebook_image_mean, codebook_image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid_images(images):\n\u001b[1;32m    659\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    660\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m     )\n\u001b[0;32m--> 664\u001b[0m processed_images \u001b[39m=\u001b[39m [\n\u001b[1;32m    665\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_image(\n\u001b[1;32m    666\u001b[0m         image\u001b[39m=\u001b[39mimg,\n\u001b[1;32m    667\u001b[0m         do_resize\u001b[39m=\u001b[39mdo_resize,\n\u001b[1;32m    668\u001b[0m         size\u001b[39m=\u001b[39msize,\n\u001b[1;32m    669\u001b[0m         resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m    670\u001b[0m         do_center_crop\u001b[39m=\u001b[39mdo_center_crop,\n\u001b[1;32m    671\u001b[0m         crop_size\u001b[39m=\u001b[39mcrop_size,\n\u001b[1;32m    672\u001b[0m         do_rescale\u001b[39m=\u001b[39mdo_rescale,\n\u001b[1;32m    673\u001b[0m         rescale_factor\u001b[39m=\u001b[39mrescale_factor,\n\u001b[1;32m    674\u001b[0m         do_normalize\u001b[39m=\u001b[39mdo_normalize,\n\u001b[1;32m    675\u001b[0m         image_mean\u001b[39m=\u001b[39mimage_mean,\n\u001b[1;32m    676\u001b[0m         image_std\u001b[39m=\u001b[39mimage_std,\n\u001b[1;32m    677\u001b[0m         do_map_pixels\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    678\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    680\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images\n\u001b[1;32m    681\u001b[0m ]\n\u001b[1;32m    682\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: processed_images}\n\u001b[1;32m    684\u001b[0m \u001b[39mif\u001b[39;00m return_codebook_pixels:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/image_processing_flava.py:665\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid_images(images):\n\u001b[1;32m    659\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    660\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m     )\n\u001b[1;32m    664\u001b[0m processed_images \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 665\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_image(\n\u001b[1;32m    666\u001b[0m         image\u001b[39m=\u001b[39;49mimg,\n\u001b[1;32m    667\u001b[0m         do_resize\u001b[39m=\u001b[39;49mdo_resize,\n\u001b[1;32m    668\u001b[0m         size\u001b[39m=\u001b[39;49msize,\n\u001b[1;32m    669\u001b[0m         resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m    670\u001b[0m         do_center_crop\u001b[39m=\u001b[39;49mdo_center_crop,\n\u001b[1;32m    671\u001b[0m         crop_size\u001b[39m=\u001b[39;49mcrop_size,\n\u001b[1;32m    672\u001b[0m         do_rescale\u001b[39m=\u001b[39;49mdo_rescale,\n\u001b[1;32m    673\u001b[0m         rescale_factor\u001b[39m=\u001b[39;49mrescale_factor,\n\u001b[1;32m    674\u001b[0m         do_normalize\u001b[39m=\u001b[39;49mdo_normalize,\n\u001b[1;32m    675\u001b[0m         image_mean\u001b[39m=\u001b[39;49mimage_mean,\n\u001b[1;32m    676\u001b[0m         image_std\u001b[39m=\u001b[39;49mimage_std,\n\u001b[1;32m    677\u001b[0m         do_map_pixels\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    678\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    680\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images\n\u001b[1;32m    681\u001b[0m ]\n\u001b[1;32m    682\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: processed_images}\n\u001b[1;32m    684\u001b[0m \u001b[39mif\u001b[39;00m return_codebook_pixels:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/image_processing_flava.py:471\u001b[0m, in \u001b[0;36mFlavaImageProcessor._preprocess_image\u001b[0;34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_map_pixels, data_format)\u001b[0m\n\u001b[1;32m    468\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor)\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 471\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize(image\u001b[39m=\u001b[39;49mimage, mean\u001b[39m=\u001b[39;49mimage_mean, std\u001b[39m=\u001b[39;49mimage_std)\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m do_map_pixels:\n\u001b[1;32m    474\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_pixels(image)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/flava/image_processing_flava.py:427\u001b[0m, in \u001b[0;36mFlavaImageProcessor.normalize\u001b[0;34m(self, image, mean, std, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize\u001b[39m(\n\u001b[1;32m    407\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    408\u001b[0m     image: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    413\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    414\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39m            The channel dimension format of the image. If not provided, it will be the same as the input image.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     \u001b[39mreturn\u001b[39;00m normalize(image, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, data_format\u001b[39m=\u001b[39;49mdata_format, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/image_transforms.py:369\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(image, mean, std, data_format)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mean, Iterable):\n\u001b[1;32m    368\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mean) \u001b[39m!=\u001b[39m num_channels:\n\u001b[0;32m--> 369\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean must have \u001b[39m\u001b[39m{\u001b[39;00mnum_channels\u001b[39m}\u001b[39;00m\u001b[39m elements if it is an iterable, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(mean)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    370\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     mean \u001b[39m=\u001b[39m [mean] \u001b[39m*\u001b[39m num_channels\n",
      "\u001b[0;31mValueError\u001b[0m: mean must have 1 elements if it is an iterable, got 3"
     ]
    }
   ],
   "source": [
    "# random torch tensor image\n",
    "import torch\n",
    "\n",
    "audio_image = torch.rand(1, 80, 200)\n",
    "\n",
    "out = processor(images=audio_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/Users/lukas/Desktop/Projects/MIT/MIT_prosody/data/audio_debug\"\n",
    "\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1037, 6302, 1997, 1037, 4937,  102],\n",
       "        [ 101, 1037, 6302, 1997, 1037, 3899,  102]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"facebook/flava-full\")\n",
    "inputs = tokenizer(\n",
    "    [\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    "    max_length=10,\n",
    ")\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "token_type_ids = inputs.token_type_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_tokens(inputs, attention_mask=None, mask_prob=0.3):\n",
    "    labels = inputs.clone()\n",
    "\n",
    "    # Probability matrix should only allow masking where attention_mask is 1\n",
    "    if attention_mask is not None:\n",
    "        # Use the attention_mask to limit where tokens can be masked\n",
    "        probability_matrix = torch.full(labels.shape, mask_prob) * attention_mask\n",
    "    else:\n",
    "        # If no attention_mask is provided, tokens can be masked anywhere\n",
    "        probability_matrix = torch.full(labels.shape, mask_prob)\n",
    "\n",
    "    # Determine which tokens to mask\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "\n",
    "    # Mask tokens\n",
    "    inputs[masked_indices] = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "\n",
    "    # Replace -100 in labels that we do not want to compute the loss for\n",
    "    labels[~masked_indices] = -100\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 103, 1037, 6302, 1997, 1037,  103,  102],\n",
       "         [ 103,  103,  103, 1997, 1037,  103,  102]]),\n",
       " tensor([[ 101, -100, -100, -100, -100, 4937, -100],\n",
       "         [ 101, 1037, 6302, -100, -100, 3899, -100]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_inputs, labels = _mask_tokens(input_ids, attention_mask=attention_mask)\n",
    "masked_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    # input_ids=input_ids,\n",
    "    input_ids_masked=masked_inputs,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    "    mlm_labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1305, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flava text model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaTextModel: ['flava.image_model.encoder.layer.5.attention.attention.query.weight', 'mlm_head.transform.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.query.bias', 'flava.image_model.encoder.layer.1.attention.output.dense.bias', 'mmm_image_head.bias', 'flava.image_model.encoder.layer.8.attention.output.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.1.layernorm_after.weight', 'flava.image_model.encoder.layer.10.attention.attention.key.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.4.attention.output.dense.weight', 'flava.image_model.encoder.layer.3.layernorm_after.bias', 'flava.image_model.encoder.layer.1.attention.attention.query.bias', 'flava.image_model.encoder.layer.6.attention.attention.key.weight', 'flava.image_model.encoder.layer.11.attention.attention.value.weight', 'flava.image_model.encoder.layer.4.output.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.1.output.dense.bias', 'flava.image_model.encoder.layer.2.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.value.weight', 'flava.image_model.encoder.layer.0.layernorm_before.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'flava.image_model.embeddings.cls_token', 'flava.image_model.encoder.layer.5.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.3.layernorm_before.weight', 'flava.image_model.encoder.layer.4.output.dense.bias', 'flava.image_model.encoder.layer.5.attention.attention.key.bias', 'flava.text_to_mm_projection.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.key.weight', 'flava.image_model.encoder.layer.6.attention.attention.value.weight', 'flava.image_model.encoder.layer.7.attention.attention.key.weight', 'flava.image_model.encoder.layer.10.layernorm_after.weight', 'flava.image_model.encoder.layer.10.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.4.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.query.bias', 'flava.image_model.encoder.layer.3.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.key.bias', 'flava.image_model.encoder.layer.11.attention.attention.key.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'flava.multimodal_model.layernorm.bias', 'flava.image_model.encoder.layer.9.layernorm_before.weight', 'flava.image_model.encoder.layer.9.layernorm_after.weight', 'flava.image_model.encoder.layer.8.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.layernorm_before.bias', 'flava.image_model.encoder.layer.7.attention.attention.value.weight', 'flava.image_model.encoder.layer.5.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.query.bias', 'flava.image_model.encoder.layer.3.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.1.attention.attention.key.bias', 'flava.image_model.encoder.layer.2.intermediate.dense.bias', 'flava.image_model.encoder.layer.2.layernorm_after.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.1.layernorm_before.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.5.attention.attention.key.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'mmm_text_head.bias', 'flava.multimodal_model.encoder.layer.2.intermediate.dense.bias', 'flava.image_model.encoder.layer.6.attention.attention.key.bias', 'flava.image_model.encoder.layer.3.layernorm_before.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.9.layernorm_after.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.4.intermediate.dense.weight', 'flava.image_model.encoder.layer.5.attention.attention.query.bias', 'flava.image_model.encoder.layer.9.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.5.layernorm_after.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'flava.text_projection.weight', 'flava.image_model.encoder.layer.6.layernorm_before.weight', 'mim_head.transform.LayerNorm.bias', 'flava.image_model.encoder.layer.3.attention.attention.query.bias', 'flava.image_model.encoder.layer.1.output.dense.weight', 'flava.image_model.encoder.layer.9.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'mim_head.transform.dense.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_after.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.6.attention.attention.value.bias', 'flava.image_model.encoder.layer.4.attention.attention.key.bias', 'flava.image_model.encoder.layer.0.attention.attention.value.bias', 'flava.image_model.encoder.layer.6.layernorm_after.weight', 'flava.image_model.encoder.layer.7.intermediate.dense.weight', 'flava.image_model.pooler.dense.bias', 'flava.multimodal_model.encoder.layer.0.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_before.bias', 'mmm_text_head.transform.dense.bias', 'flava.multimodal_model.encoder.layer.2.intermediate.dense.weight', 'flava.image_model.embeddings.patch_embeddings.projection.bias', 'flava.image_projection.bias', 'flava.image_model.encoder.layer.10.layernorm_before.bias', 'mmm_image_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.8.output.dense.bias', 'mmm_text_head.transform.LayerNorm.weight', 'flava.image_model.encoder.layer.8.layernorm_before.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.5.output.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.3.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.8.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.value.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.1.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.query.weight', 'flava.image_model.encoder.layer.0.attention.attention.key.weight', 'flava.image_model.encoder.layer.11.attention.output.dense.bias', 'flava.image_model.encoder.layer.0.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.2.output.dense.bias', 'mmm_image_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'flava.image_model.encoder.layer.0.intermediate.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.3.output.dense.bias', 'flava.image_model.embeddings.mask_token', 'flava.image_model.encoder.layer.9.layernorm_before.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.key.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.10.output.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.0.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_before.bias', 'mmm_text_head.decoder.weight', 'mmm_text_head.transform.dense.weight', 'itm_head.pooler.dense.weight', 'flava.image_model.encoder.layer.0.attention.attention.query.weight', 'mlm_head.transform.LayerNorm.weight', 'flava.image_model.encoder.layer.2.attention.output.dense.weight', 'flava.image_model.layernorm.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_before.weight', 'flava.image_model.encoder.layer.7.attention.attention.query.weight', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.10.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_after.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'flava.image_model.encoder.layer.10.attention.attention.query.weight', 'flava.image_model.encoder.layer.5.attention.output.dense.bias', 'flava.image_model.encoder.layer.3.output.dense.bias', 'flava.image_model.encoder.layer.7.layernorm_after.weight', 'flava.image_model.encoder.layer.0.layernorm_after.bias', 'flava.image_model.encoder.layer.7.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.key.weight', 'flava.logit_scale', 'flava.image_model.encoder.layer.0.attention.output.dense.bias', 'flava.image_model.encoder.layer.8.intermediate.dense.bias', 'mim_head.bias', 'flava.image_model.encoder.layer.7.attention.output.dense.bias', 'flava.image_model.encoder.layer.10.attention.output.dense.weight', 'flava.image_model.encoder.layer.8.attention.attention.query.weight', 'flava.image_model.encoder.layer.8.attention.attention.key.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.9.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_after.weight', 'mmm_text_head.decoder.bias', 'flava.multimodal_model.encoder.layer.3.intermediate.dense.weight', 'flava.image_model.encoder.layer.7.layernorm_before.bias', 'flava.image_model.encoder.layer.9.attention.attention.key.bias', 'flava.image_model.encoder.layer.6.attention.attention.query.weight', 'flava.image_model.encoder.layer.8.intermediate.dense.weight', 'flava.image_model.encoder.layer.0.attention.attention.key.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.2.attention.attention.key.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.8.layernorm_before.weight', 'flava.image_model.embeddings.patch_embeddings.projection.weight', 'flava.image_model.encoder.layer.2.layernorm_after.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.6.attention.attention.query.bias', 'flava.image_model.encoder.layer.0.attention.output.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.8.attention.output.dense.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.11.intermediate.dense.weight', 'flava.image_model.encoder.layer.0.output.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.2.output.dense.bias', 'flava.multimodal_model.encoder.layer.2.attention.output.dense.weight', 'flava.image_model.encoder.layer.11.attention.attention.query.bias', 'flava.image_model.encoder.layer.9.attention.attention.query.weight', 'flava.image_model.encoder.layer.9.attention.attention.query.bias', 'flava.image_model.encoder.layer.6.intermediate.dense.weight', 'flava.image_model.encoder.layer.10.output.dense.bias', 'flava.image_model.pooler.dense.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'flava.image_model.encoder.layer.2.intermediate.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'flava.multimodal_model.layernorm.weight', 'flava.image_model.encoder.layer.3.attention.attention.key.bias', 'mim_head.decoder.weight', 'flava.multimodal_model.encoder.layer.1.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.5.attention.output.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.11.attention.attention.key.bias', 'flava.multimodal_model.cls_token', 'flava.image_model.encoder.layer.3.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_before.weight', 'flava.image_model.encoder.layer.4.layernorm_before.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.0.output.dense.bias', 'mlm_head.transform.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.5.intermediate.dense.weight', 'flava.image_model.encoder.layer.8.layernorm_after.bias', 'flava.image_model.encoder.layer.3.attention.attention.value.weight', 'flava.image_model.encoder.layer.5.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.5.attention.output.dense.weight', 'flava.image_to_mm_projection.bias', 'flava.multimodal_model.encoder.layer.3.attention.output.dense.bias', 'flava.image_model.encoder.layer.10.layernorm_after.bias', 'flava.text_to_mm_projection.bias', 'flava.multimodal_model.encoder.layer.5.output.dense.weight', 'flava.image_model.encoder.layer.10.intermediate.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.1.attention.attention.key.weight', 'flava.image_model.encoder.layer.5.layernorm_before.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.10.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.1.layernorm_before.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'flava.multimodal_model.encoder.layer.1.intermediate.dense.weight', 'mlm_head.bias', 'flava.image_model.encoder.layer.1.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.5.layernorm_after.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'mmm_image_head.decoder.bias', 'flava.image_model.encoder.layer.6.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.key.bias', 'mmm_image_head.transform.LayerNorm.bias', 'flava.multimodal_model.encoder.layer.1.intermediate.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.5.intermediate.dense.bias', 'flava.multimodal_model.pooler.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'flava.multimodal_model.encoder.layer.3.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.query.weight', 'flava.image_model.encoder.layer.6.output.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.0.layernorm_before.weight', 'itm_head.pooler.dense.bias', 'flava.image_model.encoder.layer.11.attention.output.dense.weight', 'flava.image_model.encoder.layer.0.layernorm_after.weight', 'flava.image_model.encoder.layer.11.attention.attention.query.weight', 'flava.image_model.encoder.layer.7.attention.output.dense.weight', 'flava.image_model.encoder.layer.8.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.1.attention.output.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.4.intermediate.dense.bias', 'flava.image_model.encoder.layer.10.layernorm_before.weight', 'mlm_head.transform.LayerNorm.bias', 'flava.image_model.encoder.layer.11.output.dense.bias', 'flava.multimodal_model.encoder.layer.1.attention.attention.value.bias', 'flava.image_model.encoder.layer.9.intermediate.dense.weight', 'mlm_head.decoder.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_after.bias', 'flava.image_model.encoder.layer.7.intermediate.dense.bias', 'flava.image_model.encoder.layer.7.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.2.output.dense.weight', 'flava.image_model.encoder.layer.4.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.value.weight', 'flava.image_model.encoder.layer.0.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.0.layernorm_before.bias', 'flava.image_model.encoder.layer.11.intermediate.dense.bias', 'flava.image_model.encoder.layer.3.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.2.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.0.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.key.weight', 'flava.image_model.encoder.layer.2.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.query.bias', 'flava.image_model.encoder.layer.5.output.dense.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.1.attention.attention.query.weight', 'flava.image_model.encoder.layer.3.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.4.attention.output.dense.weight', 'flava.image_model.encoder.layer.9.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.key.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.1.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.0.intermediate.dense.weight', 'mim_head.transform.LayerNorm.weight', 'flava.image_model.encoder.layer.8.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.output.conv.bias', 'flava.image_model.encoder.layer.8.attention.attention.key.weight', 'flava.image_model.encoder.layer.11.layernorm_after.bias', 'flava.image_model.encoder.layer.7.attention.attention.query.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'flava.multimodal_model.encoder.layer.0.output.dense.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.1.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.value.bias', 'flava.image_model.embeddings.position_embeddings', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.5.layernorm_after.weight', 'flava.image_model.encoder.layer.6.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.0.attention.output.dense.weight', 'flava.image_model.encoder.layer.2.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.5.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.5.layernorm_before.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'itm_head.seq_relationship.bias', 'flava.multimodal_model.encoder.layer.1.layernorm_after.bias', 'flava.image_model.encoder.layer.7.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.1.output.dense.bias', 'flava.image_model.encoder.layer.4.layernorm_before.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.3.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_after.weight', 'image_codebook.blocks.output.conv.weight', 'flava.image_model.encoder.layer.4.attention.attention.query.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.11.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.4.output.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.value.bias', 'flava.image_model.encoder.layer.8.attention.attention.value.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.7.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'mmm_text_head.transform.LayerNorm.bias', 'flava.multimodal_model.encoder.layer.0.layernorm_before.weight', 'flava.image_model.encoder.layer.3.intermediate.dense.bias', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.7.output.dense.bias', 'flava.image_model.encoder.layer.3.layernorm_before.bias', 'flava.image_model.encoder.layer.10.attention.attention.query.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.7.layernorm_before.weight', 'flava.image_model.encoder.layer.2.attention.output.dense.bias', 'flava.image_to_mm_projection.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.query.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.9.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.4.intermediate.dense.bias', 'flava.image_model.encoder.layer.1.intermediate.dense.weight', 'mim_head.decoder.bias', 'flava.image_model.encoder.layer.9.attention.attention.key.weight', 'flava.multimodal_model.pooler.dense.bias', 'flava.image_model.encoder.layer.6.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.value.bias', 'flava.image_model.encoder.layer.2.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.output.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.5.output.dense.bias', 'flava.image_model.encoder.layer.10.attention.attention.value.weight', 'flava.image_model.encoder.layer.2.attention.attention.value.bias', 'flava.image_model.encoder.layer.6.output.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.value.weight', 'flava.image_model.encoder.layer.2.attention.attention.query.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'flava.image_projection.weight', 'flava.image_model.encoder.layer.6.attention.output.dense.weight', 'flava.image_model.encoder.layer.9.output.dense.weight', 'image_codebook.blocks.input.bias', 'flava.image_model.encoder.layer.0.output.dense.bias', 'flava.image_model.encoder.layer.5.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.3.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_after.bias', 'mmm_image_head.transform.LayerNorm.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_after.bias', 'flava.image_model.encoder.layer.2.layernorm_before.bias', 'flava.image_model.encoder.layer.1.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.intermediate.dense.bias', 'flava.image_model.encoder.layer.10.attention.attention.value.bias', 'flava.image_model.encoder.layer.4.intermediate.dense.weight', 'flava.image_model.encoder.layer.6.layernorm_before.bias', 'flava.image_model.encoder.layer.11.attention.attention.value.bias', 'flava.image_model.layernorm.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.value.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'flava.multimodal_model.encoder.layer.5.intermediate.dense.bias', 'flava.image_model.encoder.layer.11.layernorm_before.bias', 'flava.image_model.encoder.layer.3.attention.output.dense.weight', 'flava.image_model.encoder.layer.1.layernorm_before.bias', 'flava.image_model.encoder.layer.4.layernorm_after.bias', 'flava.text_projection.bias', 'flava.multimodal_model.encoder.layer.1.output.dense.weight', 'flava.image_model.encoder.layer.5.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.key.weight', 'flava.image_model.encoder.layer.4.attention.output.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.9.output.dense.bias', 'flava.image_model.encoder.layer.11.layernorm_after.weight', 'flava.image_model.encoder.layer.5.layernorm_before.weight', 'itm_head.seq_relationship.weight', 'flava.image_model.encoder.layer.2.attention.attention.value.weight', 'flava.image_model.encoder.layer.11.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.key.bias', 'image_codebook.blocks.input.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_before.bias', 'mim_head.transform.dense.bias', 'flava.image_model.encoder.layer.1.attention.attention.value.bias', 'flava.image_model.encoder.layer.4.layernorm_after.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight']\n",
      "- This IS expected if you are initializing FlavaTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, FlavaTextModel\n",
    "\n",
    "model = FlavaTextModel.from_pretrained(\"facebook/flava-full\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text=[\"a photo of a dog\", \"a photo of a cat or so \"],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "\n",
    "mlm_head = torch.nn.Linear(model.config.hidden_size, model.config.vocab_size, bias=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "text_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaTextModel: ['flava.image_model.encoder.layer.5.attention.attention.query.weight', 'mlm_head.transform.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.query.bias', 'flava.image_model.encoder.layer.1.attention.output.dense.bias', 'mmm_image_head.bias', 'flava.image_model.encoder.layer.8.attention.output.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.1.layernorm_after.weight', 'flava.image_model.encoder.layer.10.attention.attention.key.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.4.attention.output.dense.weight', 'flava.image_model.encoder.layer.3.layernorm_after.bias', 'flava.image_model.encoder.layer.1.attention.attention.query.bias', 'flava.image_model.encoder.layer.6.attention.attention.key.weight', 'flava.image_model.encoder.layer.11.attention.attention.value.weight', 'flava.image_model.encoder.layer.4.output.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.1.output.dense.bias', 'flava.image_model.encoder.layer.2.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.value.weight', 'flava.image_model.encoder.layer.0.layernorm_before.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'flava.image_model.embeddings.cls_token', 'flava.image_model.encoder.layer.5.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.3.layernorm_before.weight', 'flava.image_model.encoder.layer.4.output.dense.bias', 'flava.image_model.encoder.layer.5.attention.attention.key.bias', 'flava.text_to_mm_projection.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.key.weight', 'flava.image_model.encoder.layer.6.attention.attention.value.weight', 'flava.image_model.encoder.layer.7.attention.attention.key.weight', 'flava.image_model.encoder.layer.10.layernorm_after.weight', 'flava.image_model.encoder.layer.10.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.4.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.query.bias', 'flava.image_model.encoder.layer.3.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.key.bias', 'flava.image_model.encoder.layer.11.attention.attention.key.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'flava.multimodal_model.layernorm.bias', 'flava.image_model.encoder.layer.9.layernorm_before.weight', 'flava.image_model.encoder.layer.9.layernorm_after.weight', 'flava.image_model.encoder.layer.8.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.layernorm_before.bias', 'flava.image_model.encoder.layer.7.attention.attention.value.weight', 'flava.image_model.encoder.layer.5.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.query.bias', 'flava.image_model.encoder.layer.3.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.1.attention.attention.key.bias', 'flava.image_model.encoder.layer.2.intermediate.dense.bias', 'flava.image_model.encoder.layer.2.layernorm_after.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.1.layernorm_before.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.5.attention.attention.key.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'mmm_text_head.bias', 'flava.multimodal_model.encoder.layer.2.intermediate.dense.bias', 'flava.image_model.encoder.layer.6.attention.attention.key.bias', 'flava.image_model.encoder.layer.3.layernorm_before.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.9.layernorm_after.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.4.intermediate.dense.weight', 'flava.image_model.encoder.layer.5.attention.attention.query.bias', 'flava.image_model.encoder.layer.9.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.5.layernorm_after.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'flava.text_projection.weight', 'flava.image_model.encoder.layer.6.layernorm_before.weight', 'mim_head.transform.LayerNorm.bias', 'flava.image_model.encoder.layer.3.attention.attention.query.bias', 'flava.image_model.encoder.layer.1.output.dense.weight', 'flava.image_model.encoder.layer.9.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'mim_head.transform.dense.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_after.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.6.attention.attention.value.bias', 'flava.image_model.encoder.layer.4.attention.attention.key.bias', 'flava.image_model.encoder.layer.0.attention.attention.value.bias', 'flava.image_model.encoder.layer.6.layernorm_after.weight', 'flava.image_model.encoder.layer.7.intermediate.dense.weight', 'flava.image_model.pooler.dense.bias', 'flava.multimodal_model.encoder.layer.0.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_before.bias', 'mmm_text_head.transform.dense.bias', 'flava.multimodal_model.encoder.layer.2.intermediate.dense.weight', 'flava.image_model.embeddings.patch_embeddings.projection.bias', 'flava.image_projection.bias', 'flava.image_model.encoder.layer.10.layernorm_before.bias', 'mmm_image_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.8.output.dense.bias', 'mmm_text_head.transform.LayerNorm.weight', 'flava.image_model.encoder.layer.8.layernorm_before.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.5.output.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.3.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.8.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.value.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.1.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.query.weight', 'flava.image_model.encoder.layer.0.attention.attention.key.weight', 'flava.image_model.encoder.layer.11.attention.output.dense.bias', 'flava.image_model.encoder.layer.0.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.2.output.dense.bias', 'mmm_image_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'flava.image_model.encoder.layer.0.intermediate.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.3.output.dense.bias', 'flava.image_model.embeddings.mask_token', 'flava.image_model.encoder.layer.9.layernorm_before.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.key.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.10.output.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.0.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_before.bias', 'mmm_text_head.decoder.weight', 'mmm_text_head.transform.dense.weight', 'itm_head.pooler.dense.weight', 'flava.image_model.encoder.layer.0.attention.attention.query.weight', 'mlm_head.transform.LayerNorm.weight', 'flava.image_model.encoder.layer.2.attention.output.dense.weight', 'flava.image_model.layernorm.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_before.weight', 'flava.image_model.encoder.layer.7.attention.attention.query.weight', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.10.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_after.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'flava.image_model.encoder.layer.10.attention.attention.query.weight', 'flava.image_model.encoder.layer.5.attention.output.dense.bias', 'flava.image_model.encoder.layer.3.output.dense.bias', 'flava.image_model.encoder.layer.7.layernorm_after.weight', 'flava.image_model.encoder.layer.0.layernorm_after.bias', 'flava.image_model.encoder.layer.7.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.key.weight', 'flava.logit_scale', 'flava.image_model.encoder.layer.0.attention.output.dense.bias', 'flava.image_model.encoder.layer.8.intermediate.dense.bias', 'mim_head.bias', 'flava.image_model.encoder.layer.7.attention.output.dense.bias', 'flava.image_model.encoder.layer.10.attention.output.dense.weight', 'flava.image_model.encoder.layer.8.attention.attention.query.weight', 'flava.image_model.encoder.layer.8.attention.attention.key.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.9.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_after.weight', 'mmm_text_head.decoder.bias', 'flava.multimodal_model.encoder.layer.3.intermediate.dense.weight', 'flava.image_model.encoder.layer.7.layernorm_before.bias', 'flava.image_model.encoder.layer.9.attention.attention.key.bias', 'flava.image_model.encoder.layer.6.attention.attention.query.weight', 'flava.image_model.encoder.layer.8.intermediate.dense.weight', 'flava.image_model.encoder.layer.0.attention.attention.key.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.2.attention.attention.key.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.8.layernorm_before.weight', 'flava.image_model.embeddings.patch_embeddings.projection.weight', 'flava.image_model.encoder.layer.2.layernorm_after.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.6.attention.attention.query.bias', 'flava.image_model.encoder.layer.0.attention.output.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.8.attention.output.dense.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.11.intermediate.dense.weight', 'flava.image_model.encoder.layer.0.output.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.2.output.dense.bias', 'flava.multimodal_model.encoder.layer.2.attention.output.dense.weight', 'flava.image_model.encoder.layer.11.attention.attention.query.bias', 'flava.image_model.encoder.layer.9.attention.attention.query.weight', 'flava.image_model.encoder.layer.9.attention.attention.query.bias', 'flava.image_model.encoder.layer.6.intermediate.dense.weight', 'flava.image_model.encoder.layer.10.output.dense.bias', 'flava.image_model.pooler.dense.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'flava.image_model.encoder.layer.2.intermediate.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'flava.multimodal_model.layernorm.weight', 'flava.image_model.encoder.layer.3.attention.attention.key.bias', 'mim_head.decoder.weight', 'flava.multimodal_model.encoder.layer.1.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.5.attention.output.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.11.attention.attention.key.bias', 'flava.multimodal_model.cls_token', 'flava.image_model.encoder.layer.3.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_before.weight', 'flava.image_model.encoder.layer.4.layernorm_before.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.0.output.dense.bias', 'mlm_head.transform.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.5.intermediate.dense.weight', 'flava.image_model.encoder.layer.8.layernorm_after.bias', 'flava.image_model.encoder.layer.3.attention.attention.value.weight', 'flava.image_model.encoder.layer.5.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.5.attention.output.dense.weight', 'flava.image_to_mm_projection.bias', 'flava.multimodal_model.encoder.layer.3.attention.output.dense.bias', 'flava.image_model.encoder.layer.10.layernorm_after.bias', 'flava.text_to_mm_projection.bias', 'flava.multimodal_model.encoder.layer.5.output.dense.weight', 'flava.image_model.encoder.layer.10.intermediate.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.1.attention.attention.key.weight', 'flava.image_model.encoder.layer.5.layernorm_before.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.10.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.1.layernorm_before.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'flava.multimodal_model.encoder.layer.1.intermediate.dense.weight', 'mlm_head.bias', 'flava.image_model.encoder.layer.1.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.5.layernorm_after.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'mmm_image_head.decoder.bias', 'flava.image_model.encoder.layer.6.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.key.bias', 'mmm_image_head.transform.LayerNorm.bias', 'flava.multimodal_model.encoder.layer.1.intermediate.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.5.intermediate.dense.bias', 'flava.multimodal_model.pooler.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'flava.multimodal_model.encoder.layer.3.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.query.weight', 'flava.image_model.encoder.layer.6.output.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.0.layernorm_before.weight', 'itm_head.pooler.dense.bias', 'flava.image_model.encoder.layer.11.attention.output.dense.weight', 'flava.image_model.encoder.layer.0.layernorm_after.weight', 'flava.image_model.encoder.layer.11.attention.attention.query.weight', 'flava.image_model.encoder.layer.7.attention.output.dense.weight', 'flava.image_model.encoder.layer.8.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.1.attention.output.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.4.intermediate.dense.bias', 'flava.image_model.encoder.layer.10.layernorm_before.weight', 'mlm_head.transform.LayerNorm.bias', 'flava.image_model.encoder.layer.11.output.dense.bias', 'flava.multimodal_model.encoder.layer.1.attention.attention.value.bias', 'flava.image_model.encoder.layer.9.intermediate.dense.weight', 'mlm_head.decoder.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_after.bias', 'flava.image_model.encoder.layer.7.intermediate.dense.bias', 'flava.image_model.encoder.layer.7.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.2.output.dense.weight', 'flava.image_model.encoder.layer.4.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.value.weight', 'flava.image_model.encoder.layer.0.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.0.layernorm_before.bias', 'flava.image_model.encoder.layer.11.intermediate.dense.bias', 'flava.image_model.encoder.layer.3.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.2.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.0.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.key.weight', 'flava.image_model.encoder.layer.2.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.query.bias', 'flava.image_model.encoder.layer.5.output.dense.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.1.attention.attention.query.weight', 'flava.image_model.encoder.layer.3.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.4.attention.output.dense.weight', 'flava.image_model.encoder.layer.9.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.key.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.1.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.0.intermediate.dense.weight', 'mim_head.transform.LayerNorm.weight', 'flava.image_model.encoder.layer.8.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.output.conv.bias', 'flava.image_model.encoder.layer.8.attention.attention.key.weight', 'flava.image_model.encoder.layer.11.layernorm_after.bias', 'flava.image_model.encoder.layer.7.attention.attention.query.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'flava.multimodal_model.encoder.layer.0.output.dense.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.1.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.value.bias', 'flava.image_model.embeddings.position_embeddings', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.5.layernorm_after.weight', 'flava.image_model.encoder.layer.6.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.0.attention.output.dense.weight', 'flava.image_model.encoder.layer.2.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.5.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.5.layernorm_before.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'itm_head.seq_relationship.bias', 'flava.multimodal_model.encoder.layer.1.layernorm_after.bias', 'flava.image_model.encoder.layer.7.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.1.output.dense.bias', 'flava.image_model.encoder.layer.4.layernorm_before.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.3.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_after.weight', 'image_codebook.blocks.output.conv.weight', 'flava.image_model.encoder.layer.4.attention.attention.query.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.11.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.4.output.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.value.bias', 'flava.image_model.encoder.layer.8.attention.attention.value.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.7.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'mmm_text_head.transform.LayerNorm.bias', 'flava.multimodal_model.encoder.layer.0.layernorm_before.weight', 'flava.image_model.encoder.layer.3.intermediate.dense.bias', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.7.output.dense.bias', 'flava.image_model.encoder.layer.3.layernorm_before.bias', 'flava.image_model.encoder.layer.10.attention.attention.query.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.7.layernorm_before.weight', 'flava.image_model.encoder.layer.2.attention.output.dense.bias', 'flava.image_to_mm_projection.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.query.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.9.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.4.intermediate.dense.bias', 'flava.image_model.encoder.layer.1.intermediate.dense.weight', 'mim_head.decoder.bias', 'flava.image_model.encoder.layer.9.attention.attention.key.weight', 'flava.multimodal_model.pooler.dense.bias', 'flava.image_model.encoder.layer.6.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.value.bias', 'flava.image_model.encoder.layer.2.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.output.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.5.output.dense.bias', 'flava.image_model.encoder.layer.10.attention.attention.value.weight', 'flava.image_model.encoder.layer.2.attention.attention.value.bias', 'flava.image_model.encoder.layer.6.output.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.value.weight', 'flava.image_model.encoder.layer.2.attention.attention.query.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'flava.image_projection.weight', 'flava.image_model.encoder.layer.6.attention.output.dense.weight', 'flava.image_model.encoder.layer.9.output.dense.weight', 'image_codebook.blocks.input.bias', 'flava.image_model.encoder.layer.0.output.dense.bias', 'flava.image_model.encoder.layer.5.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.3.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_after.bias', 'mmm_image_head.transform.LayerNorm.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_after.bias', 'flava.image_model.encoder.layer.2.layernorm_before.bias', 'flava.image_model.encoder.layer.1.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.intermediate.dense.bias', 'flava.image_model.encoder.layer.10.attention.attention.value.bias', 'flava.image_model.encoder.layer.4.intermediate.dense.weight', 'flava.image_model.encoder.layer.6.layernorm_before.bias', 'flava.image_model.encoder.layer.11.attention.attention.value.bias', 'flava.image_model.layernorm.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.value.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'flava.multimodal_model.encoder.layer.5.intermediate.dense.bias', 'flava.image_model.encoder.layer.11.layernorm_before.bias', 'flava.image_model.encoder.layer.3.attention.output.dense.weight', 'flava.image_model.encoder.layer.1.layernorm_before.bias', 'flava.image_model.encoder.layer.4.layernorm_after.bias', 'flava.text_projection.bias', 'flava.multimodal_model.encoder.layer.1.output.dense.weight', 'flava.image_model.encoder.layer.5.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.key.weight', 'flava.image_model.encoder.layer.4.attention.output.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.9.output.dense.bias', 'flava.image_model.encoder.layer.11.layernorm_after.weight', 'flava.image_model.encoder.layer.5.layernorm_before.weight', 'itm_head.seq_relationship.weight', 'flava.image_model.encoder.layer.2.attention.attention.value.weight', 'flava.image_model.encoder.layer.11.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.key.bias', 'image_codebook.blocks.input.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_before.bias', 'mim_head.transform.dense.bias', 'flava.image_model.encoder.layer.1.attention.attention.value.bias', 'flava.image_model.encoder.layer.4.layernorm_after.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight']\n",
      "- This IS expected if you are initializing FlavaTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# batch tokenize\n",
    "\n",
    "from transformers import BertTokenizer, FlavaTextModel\n",
    "\n",
    "model = FlavaTextModel.from_pretrained(\"facebook/flava-full\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text=[\"a photo of a dog\", \"a photo of a cat or so \"],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Spectogram Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_demo (/Users/lukas/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(200,)\n",
      "(300,)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    ASTForAudioClassification,\n",
    "    ASTFeatureExtractor,\n",
    "    ASTModel,\n",
    "    ASTConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\"\n",
    ")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "feature_extractor = ASTFeatureExtractor(\n",
    "    sampling_rate=100, max_length=250, return_attention_mask=True\n",
    ")\n",
    "ast_config = ASTConfig()\n",
    "model = ASTModel(ast_config)\n",
    "\n",
    "# random audio signal\n",
    "# Let's assume each audio sample is sampled at a rate of 16 kHz, i.e., 16000 samples per second\n",
    "sampling_rate = 100\n",
    "\n",
    "# Length of audio samples in seconds\n",
    "lengths_in_seconds = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "]  # The four audio samples will be of 1, 2, 3, and 4 seconds respectively\n",
    "\n",
    "# Creating the 4 audio samples\n",
    "audio_samples = [\n",
    "    np.random.randn(sampling_rate * length) for length in lengths_in_seconds\n",
    "]\n",
    "for sample in audio_samples:\n",
    "    print(sample.shape)\n",
    "inputs = feature_extractor(\n",
    "    audio_samples, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits\n",
    "\n",
    "# predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "# predicted_label = model.config.id2label[predicted_class_ids]\n",
    "# predicted_label\n",
    "\n",
    "# # compute loss - target_label is e.g. \"down\"\n",
    "# target_label = model.config.id2label[0]\n",
    "# inputs[\"labels\"] = torch.tensor([model.config.label2id[target_label]])\n",
    "# loss = model(**inputs).loss\n",
    "# round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_values', 'attention_mask'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 250, 128])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flava tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/flava-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='facebook/flava-full', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"facebook/flava-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flava-full-tokenizer/tokenizer_config.json',\n",
       " 'flava-full-tokenizer/special_tokens_map.json',\n",
       " 'flava-full-tokenizer/vocab.txt',\n",
       " 'flava-full-tokenizer/added_tokens.json',\n",
       " 'flava-full-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save as json\n",
    "tokenizer.save_pretrained(\"flava-full-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
