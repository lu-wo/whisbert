{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.0000e00],\n",
    "            [1.7091e-04],\n",
    "            [0.0000e00],\n",
    "            [1.2972e-05],\n",
    "            [7.4123e-05],\n",
    "            [0.0000e00],\n",
    "            [6.2838e-05],\n",
    "            [2.1668e-05],\n",
    "            [0.0000e00],\n",
    "            [7.9288e-05],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [4.1136e-05],\n",
    "            [4.6410e-05],\n",
    "            [0.0000e00],\n",
    "            [1.6944e-05],\n",
    "            [1.6733e-04],\n",
    "            [0.0000e00],\n",
    "            [2.7461e-05],\n",
    "            [0.0000e00],\n",
    "            [2.5418e-04],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [4.1384e-05],\n",
    "            [8.7086e-05],\n",
    "            [0.0000e00],\n",
    "            [5.9463e-05],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [4.9350e-03],\n",
    "            [4.1135e-04],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [5.3831e-03],\n",
    "            [7.3946e-05],\n",
    "            [7.5058e-05],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [2.8100e-05],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [5.2669e-03],\n",
    "            [0.0000e00],\n",
    "            [6.8879e-05],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [3.5623e-05],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [1.9181e-04],\n",
    "            [0.0000e00],\n",
    "            [1.5686e-04],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "            [0.0000e00],\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7091e-04, 1.2972e-05, 7.4123e-05, 6.2838e-05, 2.1668e-05, 7.9288e-05,\n",
       "        4.1136e-05, 4.6410e-05, 1.6944e-05, 1.6733e-04, 2.7461e-05, 2.5418e-04,\n",
       "        4.1384e-05, 8.7086e-05, 5.9463e-05, 4.9350e-03, 4.1135e-04, 5.3831e-03,\n",
       "        7.3946e-05, 7.5058e-05, 2.8100e-05, 5.2669e-03, 6.8879e-05, 3.5623e-05,\n",
       "        1.9181e-04, 1.5686e-04])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = preds[preds != 0.0]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.2872)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(preds.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1237)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-torch.log(preds)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch cross entropy experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5598],\n",
       "        [0.0000, 0.5251],\n",
       "        [0.6172, 0.0000],\n",
       "        [0.6078, 0.0000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16 elements, 2 classes\n",
    "logits = torch.rand(4, 2)\n",
    "logits = torch.nn.Softmax(dim=1)(logits)\n",
    "logits[logits < 0.5] = 0.0\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor([0, 1, 0, 1])\n",
    "\n",
    "labels[:2] = 0\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.6172, 0.0000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logits[torch.arange(4), labels]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7553)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(preds + 1e-5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8689)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "loss(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([ 101, 1045, 2293, 2000, 2377, 2374, 1012,  102])\n",
      "masked input_ids: tensor([ 103,  103, 2293, 2000, 2377,  103, 1012,  103])\n",
      "labels: tensor([-100, 1045, -100, 2000, 2377, 2374, -100, -100])\n",
      "Loss: 0.3133973181247711\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a sentence\n",
    "sentence = \"I love to play football.\"\n",
    "\n",
    "# Encode the sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Get the input IDs\n",
    "input_ids = inputs[\"input_ids\"].squeeze()  # squeeze to remove the batch dimension\n",
    "print(f\"input_ids: {input_ids}\")\n",
    "\n",
    "# Calculate the number of 40% tokens\n",
    "num_tokens_to_mask = int(len(input_ids) * 0.5)\n",
    "\n",
    "# Choose random tokens to mask, fix seed to reproduce results\n",
    "np.random.seed(42)\n",
    "mask_indices = np.random.choice(len(input_ids), num_tokens_to_mask, replace=False)\n",
    "\n",
    "# Clone the input_ids for labels\n",
    "labels = input_ids.clone()\n",
    "\n",
    "# Set the label of all non-masked tokens to -100\n",
    "labels[~torch.tensor(mask_indices)] = -100\n",
    "\n",
    "# Mask the chosen tokens in input_ids\n",
    "masked_input_ids = input_ids.clone()\n",
    "masked_input_ids[mask_indices] = tokenizer.mask_token_id\n",
    "\n",
    "print(f\"masked input_ids: {masked_input_ids}\")\n",
    "print(f\"labels: {labels}\")\n",
    "\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(\n",
    "    input_ids=input_ids.unsqueeze(0), labels=labels.unsqueeze(0)\n",
    ")  # unsqueeze to add the batch dimension\n",
    "\n",
    "# Print the loss\n",
    "print(f\"Loss: {outputs.loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1012, 1012, 2293, 2000, 2377, 2374, 1012, 1012]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "probs = torch.nn.Softmax(dim=2)(logits)\n",
    "indices = torch.argmax(probs, dim=2)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 1045, 2293, 2000, 2377, 2374, 1012,  102])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
