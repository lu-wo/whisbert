{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio root\n",
    "audio_root = \"/Users/lukas/Desktop/Projects/MIT/data/peoples_speech/audio_debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found 5 flac files\n",
      "Counting samples...\n",
      "Counting took 0.6520729064941406 seconds\n",
      "Dataset has 386 samples\n"
     ]
    }
   ],
   "source": [
    "from src.data.components.datasets import AudioDataset\n",
    "\n",
    "dataset = AudioDataset(root_dir=audio_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_values'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = dataset[1]\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153307,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/Users/lukas/Desktop/Projects/MIT/data/peoples_speech/debug/files.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "audio_files = df.iloc[:, 1].tolist()  # First column\n",
    "text_files = df.iloc[:, 2].tolist()  # Second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files took 0.12194991111755371 seconds\n",
      "Dataset has 977777 samples\n"
     ]
    }
   ],
   "source": [
    "from src.data.components.datasets import PeoplesMultiModalDataset\n",
    "\n",
    "dataset = PeoplesMultiModalDataset(\n",
    "    alignment_files=text_files,\n",
    "    flac_files=audio_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_txt_offset: 0, len: 2610\n"
     ]
    }
   ],
   "source": [
    "audio, text = dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_txt_offset: 90, len: 2610\n",
      "0 [tensor([[-0.0302, -0.1455,  0.1164,  ..., -0.2841,  0.1874, -0.1072]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 180, len: 2610\n",
      "1 [tensor([[ 0.0446, -0.2279,  0.2324,  ...,  0.2914,  0.2877,  0.2301]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 270, len: 2610\n",
      "2 [tensor([[-0.1417, -0.1721, -0.2033,  ..., -0.2817, -0.1458, -0.1779]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 360, len: 2610\n",
      "3 [tensor([[ 0.2317, -0.2409, -0.2714,  ...,  0.3201, -0.1947,  0.0715]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 450, len: 2610\n",
      "4 [tensor([[ 0.0645, -0.0316, -0.1630,  ...,  0.0160,  0.0107,  0.0298]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 540, len: 2610\n",
      "5 [tensor([[0.0962, 0.0623, 0.0779,  ..., 0.1184, 0.2311, 0.2968]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 630, len: 2610\n",
      "6 [tensor([[ 0.0730,  0.0884,  0.1078,  ..., -0.0422, -0.0482, -0.0522]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 720, len: 2610\n",
      "7 [tensor([[-0.1629,  0.0506, -0.1085,  ..., -0.2725, -0.2617, -0.2440]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 810, len: 2610\n",
      "8 [tensor([[-0.0063, -0.0092,  0.0166,  ..., -0.0784, -0.0945, -0.1079]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 900, len: 2610\n",
      "9 [tensor([[-0.2078, -0.2070, -0.2097,  ..., -0.5397, -0.5918, -0.6297]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 990, len: 2610\n",
      "10 [tensor([[-0.2863, -0.4006, -0.5218,  ..., -0.0217, -0.0191, -0.0210]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1080, len: 2610\n",
      "11 [tensor([[-0.0575, -0.0500, -0.0584,  ..., -0.0335, -0.0247, -0.0062]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1170, len: 2610\n",
      "12 [tensor([[-0.4447, -0.1783,  0.0816,  ...,  0.0341,  0.0423,  0.0385]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1260, len: 2610\n",
      "13 [tensor([[-0.0110, -0.0110, -0.0085,  ..., -0.1666, -0.1691, -0.1775]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1350, len: 2610\n",
      "14 [tensor([[-0.1703, -0.2017, -0.2175,  ..., -0.1735, -0.1671, -0.1333]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1440, len: 2610\n",
      "15 [tensor([[ 0.0437, -0.0340, -0.1013,  ...,  0.0917,  0.0624,  0.0849]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1530, len: 2610\n",
      "16 [tensor([[-0.4477, -0.4188, -0.3547,  ...,  0.1088,  0.1317,  0.2560]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1620, len: 2610\n",
      "17 [tensor([[ 0.0159,  0.0868,  0.1542,  ..., -0.0159, -0.0270, -0.0328]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1710, len: 2610\n",
      "18 [tensor([[ 0.1063, -0.0467, -0.0516,  ..., -0.0568, -0.0382, -0.0108]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1800, len: 2610\n",
      "19 [tensor([[ 0.0129,  0.0185,  0.0123,  ..., -0.0867, -0.0590, -0.1426]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1890, len: 2610\n",
      "20 [tensor([[ 0.1001,  0.1040,  0.1113,  ..., -0.0735, -0.0505, -0.0328]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1980, len: 2610\n",
      "21 [tensor([[ 0.3892,  0.3428,  0.3298,  ..., -0.0438, -0.0495, -0.0642]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2070, len: 2610\n",
      "22 [tensor([[-0.0052,  0.0217, -0.0331,  ...,  0.3518, -0.2834,  0.0795]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2160, len: 2610\n",
      "23 [tensor([[ 0.1549, -0.2555,  0.0511,  ...,  0.2490,  0.2196,  0.1848]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2250, len: 2610\n",
      "24 [tensor([[ 2.1362e-04,  1.0040e-02,  2.1027e-02,  ..., -2.1677e-01,\n",
      "         -1.6748e-01, -1.6638e-01]], dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2340, len: 2610\n",
      "25 [tensor([[0.2315, 0.2377, 0.2332,  ..., 0.1495, 0.1817, 0.1389]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2430, len: 2610\n",
      "26 [tensor([[-0.1758, -0.1691, -0.0779,  ..., -0.0013, -0.1600, -0.0198]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2520, len: 2610\n",
      "27 [tensor([[-0.1063, -0.2487, -0.0222,  ..., -0.2826, -0.2385, -0.1396]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2610, len: 2610\n",
      "Loading files took 0.23038506507873535 seconds\n",
      "28 [tensor([[ 0.0027,  0.0032,  0.0037,  ..., -0.0097, -0.0163, -0.0364]],\n",
      "       dtype=torch.float64), (\"You may proceed. Thank you, Your Honor. And may it please the court. Brian Miser on behalf of Joseph Ward III. Through the Uniform Code of Military Justice, Congress authorized Captain Ward to be tried by a panel of eight military officers and to be convicted with the concurrence of two-thirds of those officers. But at the time of Captain Ward's court-martial and direct appeal, Neither Congress nor any military appellate court had authorized those eight officers to employ a preponderance of the evidence standard during their deliberations or to\",)]\n",
      "curr_txt_offset: 90, len: 4510\n",
      "29 [tensor([[0.0220, 0.0233, 0.0276,  ..., 0.0431, 0.0435, 0.0432]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 180, len: 4510\n",
      "30 [tensor([[-0.0379, -0.0442, -0.0500,  ..., -0.0041, -0.0026, -0.0030]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 270, len: 4510\n",
      "31 [tensor([[ 0.0009,  0.0010,  0.0016,  ..., -0.0146, -0.0173, -0.0213]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 360, len: 4510\n",
      "32 [tensor([[0.0091, 0.0067, 0.0052,  ..., 0.0122, 0.0141, 0.0158]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 450, len: 4510\n",
      "33 [tensor([[ 0.0114,  0.0139,  0.0217,  ..., -0.0026, -0.0042, -0.0060]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 540, len: 4510\n",
      "34 [tensor([[-0.0016,  0.0021,  0.0002,  ..., -0.0061,  0.0123,  0.0134]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 630, len: 4510\n",
      "35 [tensor([[-0.0452, -0.0500, -0.0415,  ...,  0.0105,  0.0103,  0.0097]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 720, len: 4510\n",
      "36 [tensor([[-0.0095, -0.0095, -0.0095,  ..., -0.0012, -0.0013,  0.0003]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 810, len: 4510\n",
      "37 [tensor([[-0.0630, -0.0080,  0.0548,  ..., -0.0051, -0.0057, -0.0065]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 900, len: 4510\n",
      "38 [tensor([[ 0.0170,  0.0186,  0.0200,  ..., -0.0049, -0.0035, -0.0028]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 990, len: 4510\n",
      "39 [tensor([[-0.0003,  0.0009,  0.0028,  ...,  0.0289,  0.0163,  0.0120]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1080, len: 4510\n",
      "40 [tensor([[ 0.0143,  0.0178,  0.0167,  ..., -0.0019, -0.0051, -0.0102]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1170, len: 4510\n",
      "41 [tensor([[-0.0305, -0.0298, -0.0291,  ...,  0.0234,  0.0211,  0.0200]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1260, len: 4510\n",
      "42 [tensor([[-0.0069, -0.0093, -0.0030,  ...,  0.0204,  0.0219,  0.0222]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1350, len: 4510\n",
      "43 [tensor([[-0.0016, -0.0047, -0.0069,  ..., -0.0057, -0.0040, -0.0023]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1440, len: 4510\n",
      "44 [tensor([[ 0.0006,  0.0024,  0.0035,  ..., -0.0193, -0.0186, -0.0176]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1530, len: 4510\n",
      "45 [tensor([[-0.0352, -0.0330, -0.0361,  ..., -0.0063, -0.0075, -0.0065]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1620, len: 4510\n",
      "46 [tensor([[-0.0070, -0.0047,  0.0146,  ...,  0.0027,  0.0054,  0.0078]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1710, len: 4510\n",
      "47 [tensor([[ 0.0194,  0.0178,  0.0220,  ..., -0.0165, -0.0116, -0.0092]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1800, len: 4510\n",
      "48 [tensor([[ 0.0160,  0.0146,  0.0226,  ..., -0.0995, -0.1021, -0.0977]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1890, len: 4510\n",
      "49 [tensor([[0.0699, 0.0707, 0.0641,  ..., 0.0072, 0.0073, 0.0073]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1980, len: 4510\n",
      "50 [tensor([[ 0.0067,  0.0061,  0.0056,  ..., -0.0013, -0.0012, -0.0030]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2070, len: 4510\n",
      "51 [tensor([[-0.0040, -0.0011,  0.0031,  ..., -0.0019, -0.0022, -0.0028]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2160, len: 4510\n",
      "52 [tensor([[0.0116, 0.0133, 0.0135,  ..., 0.0117, 0.0101, 0.0084]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2250, len: 4510\n",
      "53 [tensor([[-0.0021, -0.0020, -0.0022,  ..., -0.0057, -0.0056, -0.0031]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2340, len: 4510\n",
      "54 [tensor([[-0.0106, -0.0197, -0.0270,  ..., -0.0299, -0.0217, -0.0108]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2430, len: 4510\n",
      "55 [tensor([[-0.0175, -0.0083,  0.0020,  ...,  0.0106,  0.0112,  0.0119]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2520, len: 4510\n",
      "56 [tensor([[ 0.0061,  0.0072,  0.0090,  ...,  0.0140,  0.0086, -0.0046]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2610, len: 4510\n",
      "57 [tensor([[ 0.0077,  0.0058,  0.0050,  ..., -0.0019, -0.0017, -0.0015]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2700, len: 4510\n",
      "58 [tensor([[-0.0113, -0.0084, -0.0075,  ..., -0.0285, -0.0389, -0.0463]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2790, len: 4510\n",
      "59 [tensor([[ 0.0028,  0.0053,  0.0077,  ..., -0.0162, -0.0143, -0.0155]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2880, len: 4510\n",
      "60 [tensor([[-0.0022, -0.0040, -0.0038,  ..., -0.0043, -0.0011,  0.0011]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 2970, len: 4510\n",
      "61 [tensor([[-0.0029, -0.0029, -0.0031,  ..., -0.0040, -0.0015,  0.0042]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3060, len: 4510\n",
      "62 [tensor([[-0.0018, -0.0031, -0.0043,  ..., -0.0016, -0.0024, -0.0044]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3150, len: 4510\n",
      "63 [tensor([[-0.0025, -0.0080, -0.0103,  ...,  0.0009,  0.0010,  0.0010]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3240, len: 4510\n",
      "64 [tensor([[-0.0097, -0.0237, -0.0262,  ...,  0.0129,  0.0090,  0.0055]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3330, len: 4510\n",
      "65 [tensor([[ 0.0063,  0.0077,  0.0088,  ..., -0.0105, -0.0085, -0.0094]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3420, len: 4510\n",
      "66 [tensor([[-0.0052, -0.0052, -0.0043,  ...,  0.0166,  0.0193,  0.0218]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3510, len: 4510\n",
      "67 [tensor([[-0.0109, -0.0042,  0.0018,  ...,  0.0136,  0.0219,  0.0390]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3600, len: 4510\n",
      "68 [tensor([[ 0.0032, -0.0027, -0.0074,  ..., -0.0860,  0.0848,  0.1480]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3690, len: 4510\n",
      "69 [tensor([[-0.0079, -0.0126, -0.0147,  ..., -0.0009, -0.0009, -0.0015]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3780, len: 4510\n",
      "70 [tensor([[ 0.0039,  0.0016,  0.0001,  ..., -0.0006, -0.0051, -0.0058]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3870, len: 4510\n",
      "71 [tensor([[ 0.0010,  0.0074,  0.0095,  ..., -0.0157, -0.0164, -0.0128]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 3960, len: 4510\n",
      "72 [tensor([[ 0.0024,  0.0037,  0.0049,  ..., -0.0002,  0.0002,  0.0010]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 4050, len: 4510\n",
      "73 [tensor([[ 0.0600,  0.0603,  0.0229,  ..., -0.0132, -0.0217, -0.0066]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 4140, len: 4510\n",
      "74 [tensor([[-0.0173, -0.0142, -0.0114,  ...,  0.0108,  0.0082,  0.0070]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 4230, len: 4510\n",
      "75 [tensor([[-0.0256, -0.0258, -0.0207,  ...,  0.0092,  0.0082,  0.0049]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 4320, len: 4510\n",
      "76 [tensor([[-0.0016, -0.0016, -0.0017,  ...,  0.0207,  0.0365,  0.0554]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 4410, len: 4510\n",
      "77 [tensor([[ 0.0366,  0.0314,  0.0190,  ..., -0.0628, -0.0565, -0.0564]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 4500, len: 4510\n",
      "Loading files took 0.08494400978088379 seconds\n",
      "78 [tensor([[-0.0052, -0.0041, -0.0052,  ..., -0.0004,  0.0124,  0.0097]],\n",
      "       dtype=torch.float64), (\"Can I proceed? Yes, you may. May it please the court, seventeen point three septillion sounds like a video game score, but it's only as impressive as the forensic lab producing it. This appeal, among others, articulating clearly for the court, in my view, the probative value and the critical need for the evidence that the judge precluded. So what you have here is a case study in that evidence because it was aired at a full blown evidentiary hearing before trial. And what did that evidence show? Well, it\",)]\n",
      "curr_txt_offset: 90, len: 1790\n",
      "79 [tensor([[-0.0130, -0.0253,  0.0017,  ..., -0.0078, -0.0085, -0.0079]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 180, len: 1790\n",
      "80 [tensor([[ 0.0208,  0.0187,  0.0148,  ..., -0.1730, -0.1600, -0.1333]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 270, len: 1790\n",
      "81 [tensor([[-0.0776, -0.0421, -0.0069,  ..., -0.0083,  0.0151,  0.0082]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 360, len: 1790\n",
      "82 [tensor([[-0.0346, -0.0400, -0.0484,  ...,  0.0117,  0.0131,  0.0137]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 450, len: 1790\n",
      "83 [tensor([[-0.0158, -0.0264, -0.0413,  ...,  0.0188,  0.0165,  0.0140]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 540, len: 1790\n",
      "84 [tensor([[0.0010, 0.0007, 0.0005,  ..., 0.0337, 0.0325, 0.0316]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 630, len: 1790\n",
      "85 [tensor([[-0.0222, -0.0250, -0.0302,  ..., -0.0041, -0.0059, -0.0058]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 720, len: 1790\n",
      "86 [tensor([[-0.0172, -0.0170, -0.0177,  ..., -0.0136, -0.0151, -0.0155]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 810, len: 1790\n",
      "87 [tensor([[-0.0800, -0.0818, -0.0918,  ...,  0.0099,  0.0103,  0.0126]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 900, len: 1790\n",
      "88 [tensor([[0.0073, 0.0071, 0.0081,  ..., 0.0028, 0.0031, 0.0038]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 990, len: 1790\n",
      "89 [tensor([[0.0102, 0.0075, 0.0032,  ..., 0.0469, 0.0304, 0.0244]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1080, len: 1790\n",
      "90 [tensor([[-0.0335, -0.0013,  0.0335,  ..., -0.0414, -0.0513, -0.0504]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1170, len: 1790\n",
      "91 [tensor([[-0.0158, -0.0651, -0.0540,  ..., -0.0040, -0.0034, -0.0006]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1260, len: 1790\n",
      "92 [tensor([[-0.0377, -0.0394, -0.0444,  ...,  0.0078,  0.0094,  0.0110]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1350, len: 1790\n",
      "93 [tensor([[-0.0005, -0.0007, -0.0004,  ..., -0.0031, -0.0013, -0.0009]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1440, len: 1790\n",
      "94 [tensor([[ 9.1553e-05, -4.2725e-04, -9.1553e-04,  ..., -2.0935e-02,\n",
      "         -2.1027e-02, -2.0599e-02]], dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1530, len: 1790\n",
      "95 [tensor([[ 0.0139,  0.0138,  0.0149,  ..., -0.0007,  0.0048, -0.0027]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1620, len: 1790\n",
      "96 [tensor([[-0.0003, -0.0055,  0.0037,  ...,  0.0021,  0.0012,  0.0008]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 1710, len: 1790\n",
      "Loading files took 0.09438896179199219 seconds\n",
      "97 [tensor([[0.0549, 0.0612, 0.0777,  ..., 0.0122, 0.0178, 0.0167]],\n",
      "       dtype=torch.float64), (\"letter to the court dated March of twenty seven thousand six and I object to that letter and ask that it be stricken because one, it's not a twenty eight J letter and two, it's in the nature of a supplemental brief. Do you disagree with the concession in that letter? Did two officers make the observation or did three? Two officers made the observation. That's all I can see. What do you want to strike it for? Because I don't think it was done properly, and because there were\",)]\n",
      "curr_txt_offset: 90, len: 2065\n",
      "98 [tensor([[-0.0578, -0.0006,  0.0216,  ...,  0.0113,  0.0145,  0.0123]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 180, len: 2065\n",
      "99 [tensor([[ 0.0025, -0.0005, -0.0032,  ...,  0.0327,  0.0316,  0.0327]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 270, len: 2065\n",
      "100 [tensor([[-0.0875, -0.0553, -0.0253,  ...,  0.0499,  0.0491,  0.0468]],\n",
      "       dtype=torch.float64), ('',)]\n",
      "curr_txt_offset: 360, len: 2065\n",
      "101 [tensor([[0.0302, 0.0263, 0.0276,  ..., 0.0152, 0.0154, 0.0169]],\n",
      "       dtype=torch.float64), ('',)]\n"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(i, batch)\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: [{'start': 351.947, 'end': 352.007, 'word': 'to', 'confidence': 0.918, 'break_before': 0.01999999999998181, 'duration': 0.060000000000002274}, {'start': 352.087, 'end': 352.287, 'word': 'put', 'confidence': 0.918, 'break_before': 0.07999999999998408, 'duration': 0.19999999999998863}, {'start': 352.327, 'end': 352.847, 'word': 'distillate', 'confidence': 0.657, 'break_before': 0.040000000000020464, 'duration': 0.5199999999999818}, {'start': 352.867, 'end': 353.188, 'word': 'property.', 'confidence': 0.931, 'break_before': 0.020000000000038654, 'duration': 0.32099999999996953}]\n",
      "to put distillate\n"
     ]
    }
   ],
   "source": [
    "def _get_text_from_dict(cnt: list, start_idx: int, end_idx: int):\n",
    "    print(f\"cnt: {cnt}\")\n",
    "    return \" \".join([c[\"word\"] for c in cnt[start_idx:end_idx]])\n",
    "\n",
    "\n",
    "cnt = [\n",
    "    {\n",
    "        \"start\": 351.947,\n",
    "        \"end\": 352.007,\n",
    "        \"word\": \"to\",\n",
    "        \"confidence\": 0.918,\n",
    "        \"break_before\": 0.01999999999998181,\n",
    "        \"duration\": 0.060000000000002274,\n",
    "    },\n",
    "    {\n",
    "        \"start\": 352.087,\n",
    "        \"end\": 352.287,\n",
    "        \"word\": \"put\",\n",
    "        \"confidence\": 0.918,\n",
    "        \"break_before\": 0.07999999999998408,\n",
    "        \"duration\": 0.19999999999998863,\n",
    "    },\n",
    "    {\n",
    "        \"start\": 352.327,\n",
    "        \"end\": 352.847,\n",
    "        \"word\": \"distillate\",\n",
    "        \"confidence\": 0.657,\n",
    "        \"break_before\": 0.040000000000020464,\n",
    "        \"duration\": 0.5199999999999818,\n",
    "    },\n",
    "    {\n",
    "        \"start\": 352.867,\n",
    "        \"end\": 353.188,\n",
    "        \"word\": \"property.\",\n",
    "        \"confidence\": 0.931,\n",
    "        \"break_before\": 0.020000000000038654,\n",
    "        \"duration\": 0.32099999999996953,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(_get_text_from_dict(cnt, 0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with packs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.data.components.datasets import (\n",
    "    PeoplesMultiModalDataset,\n",
    "    PeoplesMultiModalPackagedDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_path = \"/Users/lukas/Desktop/Projects/MIT/data/peoples_speech/debug/files.csv\"\n",
    "mapping = pd.read_csv(mapping_path)\n",
    "alignment_files = mapping.iloc[:, 2].tolist()\n",
    "flac_files = mapping.iloc[:, 1].tolist()\n",
    "\n",
    "# Initialize the parameters\n",
    "sr = 16000  # sample rate\n",
    "num_words_per_sample = 50  # number of words per sample\n",
    "dataset_total_words = 100000  # total words in the dataset\n",
    "samples_per_pack = 100  # number of samples per pack\n",
    "save_dir = \"/Users/lukas/Desktop/Projects/MIT/data/peoples_speech/packs/debug\"  # directory to save the sample pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10 alignment files\n",
      "Dataset has 2000 samples\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the dataset\n",
    "dataset = PeoplesMultiModalDataset(\n",
    "    alignment_files=alignment_files,\n",
    "    flac_files=flac_files,\n",
    "    sr=sr,\n",
    "    num_words_per_sample=num_words_per_sample,\n",
    "    dataset_total_words=dataset_total_words,\n",
    ")\n",
    "\n",
    "# Wrap the dataset with the packaged dataset class\n",
    "packaged_dataset = PeoplesMultiModalPackagedDataset(\n",
    "    dataset=dataset,\n",
    "    num_files_per_pack=samples_per_pack,\n",
    "    save_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.0363, -0.0314, -0.0170,  ...,  0.0032,  0.0021,  0.0007]],\n",
      "       dtype=torch.float64), (\"and was really born out of precedent that was preexisting at the time, the holding is explaining why quorum novus is not available in the military courts. But there's also an alternative holding that directly addresses the issue that Petitioner raised here, which is whether or not is to apply\",)]\n",
      "[tensor([[-0.0213, -0.0287, -0.0243,  ..., -0.0350,  0.0348, -0.0430]],\n",
      "       dtype=torch.float64), (\"metallic sound was heard consistent with the gun. After he ran away, within minutes thereafter, a firearm was recovered from the same place. His fingerprints weren't on the gun, is that right? That's right, Your Honor. It had one, Your Honor, but the testimony that's in the record shows that\",)]\n",
      "[tensor([[-0.0117, -0.0152, -0.0117,  ..., -0.0650, -0.0623, -0.0606]],\n",
      "       dtype=torch.float64), (\"I looked at the scope of services that you put together for Framingham, and your name was included on the resume. So are you familiar with that Framingham process? I wrote it. I wrote it. I read it. You weren't on the cover letter, that's why I asked, but your\",)]\n",
      "[tensor([[-0.0119,  0.0046, -0.0175,  ..., -0.0045, -0.0006,  0.0005]],\n",
      "       dtype=torch.float64), ('said PALS got this insurance because of the ordinance. Well, they conspicuously certainly did not have a hundred and fifty million dollars in CGL prism, again, of that motorist on the highway or that retail customer who just happens to walk into the business to buy a tank of propane.',)]\n",
      "[tensor([[-0.0016,  0.0004,  0.0007,  ..., -0.0103,  0.0110, -0.0022]],\n",
      "       dtype=torch.float64), (\"value. How do you explain the fact that you asked for a hundred and eighty six million and you got sixteen? Yeah, so there are a lot of certain points the jury might have concluded. All right, now you've gotten so many attorney warnings that you no longer have the\",)]\n",
      "[tensor([[-0.0056, -0.0309, -0.0529,  ...,  0.1697,  0.1708,  0.1814]],\n",
      "       dtype=torch.float64), (\"at his home, but there's no evidence that he actually knew about it. I'm sorry I say that I'm out of time, so I will ask the court to affirm the lower court's decisions. Thank you. Very well. Thank you for your argument. We'll hear from you in rebuttal. Thank\",)]\n",
      "[tensor([[0.2255, 0.3245, 0.4676,  ..., 0.1405, 0.0978, 0.0741]],\n",
      "       dtype=torch.float64), (\"that's where the reductive work began, and that's why, presumably, having not been in the room to listen, we have the three hundred and fifty thousand dollars. It's clear that at the time of closing... The objection initially wasn't there to your use for impeachment purposes. There was a motion\",)]\n",
      "[tensor([[0.0018, 0.0025, 0.0022,  ..., 0.0236, 0.0236, 0.0229]],\n",
      "       dtype=torch.float64), (\"further, if you want to look behind, as you say, Sanford gives that ability, that two-factor test, You examine the thoroughness of the military court's disposition, and you see that here, which is a one-sentence order, particularly from the Court of Appeals for the Armed Forces, and then you get\",)]\n",
      "[tensor([[ 0.0023, -0.0058, -0.0162,  ..., -0.0006, -0.0019, -0.0018]],\n",
      "       dtype=torch.float64), (\"may not look worse, but he looks the same. So the problem I'm having is, how does one legally make this kind of determination. You've got him before, you've got him after, but here on the twenty-second it looks like maybe he's okay and so he's okay? Okay. How do\",)]\n",
      "[tensor([[-0.0064, -0.0057, -0.0087,  ..., -0.0040,  0.0033,  0.0059]],\n",
      "       dtype=torch.float64), (\"meeting, but we'll try to coordinate that, schedule ourselves so it can be done as quickly as possible. Just, John, just another part of that. As soon as the board approves the text, we post a text version of the document on our website, black and white, PDF. Then we\",)]\n",
      "[tensor([[-0.0026, -0.0062, -0.0070,  ..., -0.0045, -0.0023, -0.0015]],\n",
      "       dtype=torch.float64), (\"provisions were breached You have you have every right to have those provisions you have every right to have them enforced, but we're looking at writing them out of your insurance policy. That's what Northwest is asking. The Northwest argues that if a statute makes insurance compulsory, that's the end\",)]\n",
      "[tensor([[-0.0589, -0.0620, -0.0597,  ...,  0.0208,  0.0298,  0.0374]],\n",
      "       dtype=torch.float64), ('hundred twenty six months. And on top of that, when he comes out, he has to pay two point three million dollars in restitution. All the assets that he had had been forfeited. And on top of that, he has a two point one million dollar judgment on it. So',)]\n",
      "[tensor([[-0.0011, -0.0094, -0.0115,  ...,  0.0308, -0.0028, -0.0280]],\n",
      "       dtype=torch.float64), (\"And it was not in the affidavit at the time. But the stop was good for the reasons that I talked about. They knew, in general, in the big picture scope of things. Aren't you overreading? And that's what the sentence says, too. And if that's the case, the whole\",)]\n",
      "[tensor([[0.0541, 0.0677, 0.0760,  ..., 0.0037, 0.0040, 0.0022]],\n",
      "       dtype=torch.float64), ('also the court of appeals for the armed forces earlier, uh, decision in right, which six times, and had they, uh, set the foundation for One of the exceptions for Form Four B, we would only be dealing with one due process violation here. Remember, the CAF on page three',)]\n",
      "[tensor([[0.0320, 0.0262, 0.0178,  ..., 0.0175, 0.0121, 0.0044]],\n",
      "       dtype=torch.float64), (\"letter? Did two officers make the observation or did three? Two officers made the observation. That's all I can see. What do you want to strike it for? Because I don't think it was done properly, and because there were two other admissions that were not made by the government\",)]\n",
      "[tensor([[-2.1667e-02, -2.0447e-02, -2.0325e-02,  ...,  9.1553e-05,\n",
      "          5.1880e-04,  2.7466e-04]], dtype=torch.float64), ('know our clients and our client organizations. And we have to understand what the key challenges are that the town of Foxborough will face over the next three to five years. And we gain an understanding of that by talking to each one of you for significant lengths of time.',)]\n",
      "[tensor([[0.0001, 0.0006, 0.0005,  ..., 0.0081, 0.0097, 0.0110]],\n",
      "       dtype=torch.float64), (\"to be from, I mean, Water and Sewer is an elected board. Planning Board is an elected board. I think we'll do the best to ask them twice and three times. But if they need to be on it, we'll do the best. Maybe plan B is if the advisory\",)]\n",
      "[tensor([[-0.0098, -0.0114, -0.0481,  ..., -0.1379, -0.1804, -0.1344]],\n",
      "       dtype=torch.float64), (\"they all are just equal, as Mr. Wilkins indicated. Sure. Well, I think the court might be able to write a little bit of a shorter opinion if it goes on the commission theory. So I'd be happy to walk the court through that one if you'd like. And just\",)]\n",
      "[tensor([[ 0.0235, -0.0246,  0.0017,  ...,  0.0146,  0.0158,  0.0146]],\n",
      "       dtype=torch.float64), (\"the personal money judgment is, is the forfeiture of substitute assets. To the extent that the direct property, the roughly three hundred thousand dollars for the real estate and the cars doesn't discharge the full two point one million, that's where the personal money judgment kicks in. So the most\",)]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Pack file does not exist. Please prepare the packs first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(packaged_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/Projects/MIT/MIT_prosody/src/data/components/datasets.py:678\u001b[0m, in \u001b[0;36mPeoplesMultiModalPackagedDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m    677\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_pack):\n\u001b[0;32m--> 678\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_pack()\n\u001b[1;32m    679\u001b[0m         index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# Reset index for the new pack\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_pack[index]\n",
      "File \u001b[0;32m~/Desktop/Projects/MIT/MIT_prosody/src/data/components/datasets.py:674\u001b[0m, in \u001b[0;36mPeoplesMultiModalPackagedDataset.load_pack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_pack_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    673\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPack file does not exist. Please prepare the packs first.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Pack file does not exist. Please prepare the packs first."
     ]
    }
   ],
   "source": [
    "# dataloader batch size 1\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(packaged_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
