{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative to Prev Prominence Task - Differential Entropy and Control functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.components.helsinki import HelsinkiProminenceExtractor\n",
    "from src.data.components.datasets import TokenTaggingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.text_processing import python_lowercase_remove_punctuation\n",
    "from src.utils.text_processing import get_wordlist_from_string\n",
    "\n",
    "# only to create a valid dataset\n",
    "dummy_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/Users/lukas/Desktop/projects/MIT/MIT_prosody/data/Helsinki\"\n",
    "SAVE_DIR = \"/Users/lukas/Desktop/projects/MIT/MIT_prosody/precomputed/predictions/emnlp/prominence_relative_prev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extractor = HelsinkiProminenceExtractor(\n",
    "    DATA_DIR,\n",
    "    \"train_360.txt\",\n",
    ")\n",
    "train_texts = train_extractor.get_all_texts()\n",
    "train_prominences = train_extractor.get_all_real_prominence()\n",
    "\n",
    "dev_extractor = HelsinkiProminenceExtractor(\n",
    "    DATA_DIR,\n",
    "    \"dev.txt\",\n",
    ")\n",
    "dev_texts = dev_extractor.get_all_texts()\n",
    "dev_prominences = dev_extractor.get_all_real_prominence()\n",
    "\n",
    "test_extractor = HelsinkiProminenceExtractor(\n",
    "    DATA_DIR,\n",
    "    \"test.txt\",\n",
    ")\n",
    "test_texts = test_extractor.get_all_texts()\n",
    "test_prominences = test_extractor.get_all_real_prominence()\n",
    "\n",
    "print(\n",
    "    f\"train_texts: {len(train_texts)}, dev_texts: {len(dev_texts)}, test_texts: {len(test_texts)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [word for text in train_texts for word in text.split()]\n",
    "dev_words = [word for text in dev_texts for word in text.split()]\n",
    "test_words = [word for text in test_texts for word in text.split()]\n",
    "\n",
    "print(\n",
    "    f\"train_words: {len(train_words)}, dev_words: {len(dev_words)}, test_words: {len(test_words)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_MEAN_PROMINENCE = np.mean([p for ps in train_prominences for p in ps if p])\n",
    "GLOBAL_MEAN_PROMINENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plots import plot_kde\n",
    "\n",
    "labels_non_nan = [p for ps in train_prominences for p in ps if p]\n",
    "\n",
    "# plot_kde(labels_non_nan, label_name=\"Absolute Prominence\", title=\"Absolute Prominence Distribution\", save_path=SAVE_DIR + \"/absolute_prominence_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TokenTaggingDataset(\n",
    "    input_texts=train_texts,\n",
    "    targets=train_prominences,\n",
    "    tokenizer=dummy_tokenizer,\n",
    "    model_name=\"gpt2\",\n",
    "    score_last_token=True,\n",
    "    relative_to_prev=True,\n",
    "    n_prev=3,\n",
    ")\n",
    "\n",
    "test_dataset = TokenTaggingDataset(\n",
    "    input_texts=test_texts,\n",
    "    targets=test_prominences,\n",
    "    tokenizer=dummy_tokenizer,\n",
    "    model_name=\"gpt2\",\n",
    "    score_last_token=True,\n",
    "    relative_to_prev=True,\n",
    "    n_prev=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "train_labels = []\n",
    "for i in range(len(train_dataset)):\n",
    "    item = train_dataset.__getitem__(i)\n",
    "    train_sentences.append(item[\"input_text\"])\n",
    "    mask = np.array(item[\"loss_mask\"])\n",
    "    labels = np.array(item[\"tokenized_labels\"])\n",
    "    valid_labels = np.array(labels[mask == 1])\n",
    "    train_labels.append(valid_labels)\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "for i in range(len(test_dataset)):\n",
    "    item = test_dataset.__getitem__(i)\n",
    "    test_sentences.append(item[\"input_text\"])\n",
    "    mask = np.array(item[\"loss_mask\"])\n",
    "    labels = np.array(item[\"tokenized_labels\"])\n",
    "    valid_labels = np.array(labels[mask == 1])\n",
    "    test_labels.append(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.text_processing import assign_labels_to_sentences\n",
    "\n",
    "all_train_words, all_train_labels = assign_labels_to_sentences(\n",
    "    train_sentences, train_labels\n",
    ")\n",
    "all_test_words, all_test_labels = assign_labels_to_sentences(\n",
    "    test_sentences, test_labels\n",
    ")\n",
    "\n",
    "print(len(all_train_words), len(all_train_labels))\n",
    "print(len(all_test_words), len(all_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plots import plot_kde\n",
    "\n",
    "# labels_non_nan = [p for ps in all_train_labels for p in ps if p]\n",
    "\n",
    "plot_kde(\n",
    "    all_train_labels,\n",
    "    label_name=\"Relative to 3 Previous Prominence\",\n",
    "    title=\"Relative Prominence Distribution\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation and Differential Entropy Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel density estimation\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "density = gaussian_kde(all_train_labels)\n",
    "\n",
    "# xs = np.linspace(0, 6, 1000)\n",
    "# plt.plot(xs, density(xs))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.approximation import monte_carlo_diff_entropy\n",
    "\n",
    "diff_entropy = monte_carlo_diff_entropy(density, all_train_labels, 1000)\n",
    "diff_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models and Control Functions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg of all words in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_difference = np.mean(\n",
    "    all_train_labels\n",
    ")  # Here, train_labels are assumed to be prominences\n",
    "print(f\"Average prominence: {avg_difference}\")\n",
    "\n",
    "# compute mse\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "predictions = [avg_difference] * len(\n",
    "    all_test_labels\n",
    ")  # all_test_labels are assumed to be prominences\n",
    "mse = mean_absolute_error(all_test_labels, predictions)\n",
    "print(f\"Mean absolute error: {mse}\")\n",
    "\n",
    "# compute r2\n",
    "r2 = r2_score(all_test_labels, predictions)\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "# compute pearson\n",
    "pearson = pearsonr(all_test_labels, predictions)\n",
    "print(f\"Pearson correlation: {pearson}\")\n",
    "\n",
    "# store predictions\n",
    "avg_test_predictions = []\n",
    "for i in range(len(all_test_words)):\n",
    "    sentence_predictions = [avg_difference] * len(all_test_words[i].split(\" \"))\n",
    "    avg_test_predictions.append(sentence_predictions)\n",
    "\n",
    "# store predictions\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "SAVE_DIR = \"./path/to/save/directory\"  # Please specify your directory path\n",
    "os.makedirs(f\"{SAVE_DIR}/avg\", exist_ok=True)\n",
    "\n",
    "with open(f\"{SAVE_DIR}/avg/pred_avg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(avg_test_predictions, f)\n",
    "\n",
    "# store texts\n",
    "with open(f\"{SAVE_DIR}/avg/texts_avg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_test_words, f)\n",
    "\n",
    "# store labels\n",
    "with open(f\"{SAVE_DIR}/avg/labels_avg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_test_labels, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus statistics: predict average per word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the words types and their respective labels\n",
    "word_prominence = {}\n",
    "for word, prominence in zip(all_train_words, all_train_labels):\n",
    "    if word not in word_prominence:\n",
    "        word_prominence[word] = []\n",
    "    word_prominence[word].append(prominence)\n",
    "\n",
    "# compute the average prominence score for each word\n",
    "word_prominence_avg = {}\n",
    "for word, prominence in word_prominence.items():\n",
    "    word_prominence_avg[word] = np.mean(prominence)\n",
    "\n",
    "# for each word in the test set, get the average prominence score\n",
    "predictions = []\n",
    "for word in all_test_words:\n",
    "    if word in word_prominence_avg:\n",
    "        predictions.append(word_prominence_avg[word])\n",
    "    else:\n",
    "        predictions.append(avg_difference)  # avg_difference needs to be defined\n",
    "\n",
    "print(f\"Length of test set: {len(all_test_labels)}\")\n",
    "print(f\"Length of predictions: {len(predictions)}\")\n",
    "\n",
    "# compute mae\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "mse = mean_absolute_error(all_test_labels, predictions)\n",
    "print(f\"Mean absolute error: {mse}\")\n",
    "\n",
    "# compute r2\n",
    "r2 = r2_score(all_test_labels, predictions)\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "# compute pearson\n",
    "pearson = pearsonr(all_test_labels, predictions)\n",
    "print(f\"Pearson correlation: {pearson}\")\n",
    "\n",
    "# store predictions\n",
    "word_test_predictions = []\n",
    "for sentence in all_test_words:\n",
    "    sentence_predictions = [\n",
    "        word_prominence_avg[word] if word in word_prominence_avg else avg_difference\n",
    "        for word in sentence.split()\n",
    "    ]\n",
    "    word_test_predictions.append(sentence_predictions)\n",
    "\n",
    "# store predictions\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "SAVE_DIR = \"./path/to/save/directory\"  # Please specify your directory path\n",
    "os.makedirs(f\"{SAVE_DIR}/wordavg\", exist_ok=True)\n",
    "\n",
    "with open(f\"{SAVE_DIR}/wordavg/pred_wordavg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_test_predictions, f)\n",
    "\n",
    "# store texts\n",
    "with open(f\"{SAVE_DIR}/wordavg/texts_wordavg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_test_words, f)\n",
    "\n",
    "# store labels\n",
    "with open(f\"{SAVE_DIR}/wordavg/labels_wordavg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_test_labels, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PATH = \"/Users/lukas/Desktop/projects/MIT/data/models/glove/glove.6B.300d.txt\"\n",
    "\n",
    "H_PARAMS = {\n",
    "    \"num_layers\": 3,\n",
    "    \"input_size\": 300,  # Update this based on the word embedding model\n",
    "    \"hidden_size\": 32,\n",
    "    \"num_labels\": 1,\n",
    "    \"dropout_probability\": 0.1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_epochs\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.baselines.control_function import ControlFunction\n",
    "\n",
    "control_function = ControlFunction(\n",
    "    word_embedding_type=\"glove\", word_embedding_path=GLOVE_PATH, hparams=H_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_function.fit(words=all_train_words, labels=all_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predictions\n",
    "\n",
    "pred = control_function.predict(all_test_words)\n",
    "# flatten the pred\n",
    "pred = [item for sublist in pred for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mae\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(all_test_labels, pred)\n",
    "print(f\"Mean absolute error: {mae}\")\n",
    "\n",
    "# compute r2\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(all_test_labels, pred)\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "# compute pearson\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson = pearsonr(np.array(all_test_labels), pred)\n",
    "print(f\"Pearson correlation: {pearson}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = control_function.word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = [word_embedding_model.get_word_embedding(w) for w in all_train_words]\n",
    "test_embeddings = [word_embedding_model.get_word_embedding(w) for w in all_test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn histgrad regressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(max_iter=500)\n",
    "hgb.fit(train_embeddings, all_train_labels)\n",
    "\n",
    "# store predictions\n",
    "pred = hgb.predict(test_embeddings)\n",
    "\n",
    "# compute mae\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(all_test_labels, pred)\n",
    "print(f\"Mean absolute error: {mae}\")\n",
    "\n",
    "# compute r2\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(all_test_labels, pred)\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "# compute pearson\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson = pearsonr(np.array(all_test_labels), pred)\n",
    "print(f\"Pearson correlation: {pearson}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
