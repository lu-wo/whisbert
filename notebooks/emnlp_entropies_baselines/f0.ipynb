{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Prominence Task - Differential Entropy and Control functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.data.components.helsinki import HelsinkiProminenceExtractor\n",
    "from src.data.components.datasets import TokenTaggingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.text_processing import python_lowercase_remove_punctuation\n",
    "from src.utils.text_processing import get_wordlist_from_string\n",
    "\n",
    "# only to create a valid dataset\n",
    "dummy_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_ROOT = \"/Users/lukas/Desktop/projects/MIT/data/LibriTTS\"\n",
    "LAB_ROOT = \"/Users/lukas/Desktop/projects/MIT/data/LibriTTSCorpusLabel\"\n",
    "PHONEME_LAB_ROOT = \"/Users/lukas/Desktop/projects/MIT/data/LibriTTSCorpusLabelPhoneme\"\n",
    "DATA_CACHE = \"/Users/lukas/Desktop/projects/MIT/data/cache\"\n",
    "\n",
    "TRAIN_FILE = \"train-clean-100\"\n",
    "VAL_FILE = \"dev-clean\"\n",
    "TEST_FILE = \"test-clean\"\n",
    "\n",
    "SAVE_DIR = \"/Users/lukas/Desktop/projects/MIT/MIT_prosody/precomputed/predictions/emnlp/f0_dct_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.f0_regression_datamodule import (\n",
    "    F0RegressionDataModule as DataModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule(\n",
    "    wav_root=WAV_ROOT,\n",
    "    lab_root=LAB_ROOT,\n",
    "    phoneme_lab_root=PHONEME_LAB_ROOT,\n",
    "    data_cache=DATA_CACHE,\n",
    "    train_file=TRAIN_FILE,\n",
    "    val_file=VAL_FILE,\n",
    "    test_file=TEST_FILE,\n",
    "    dataset_name=\"libritts\",\n",
    "    model_name=\"gpt2\",\n",
    "    f0_mode=\"dct\",\n",
    "    f0_n_coeffs=4,\n",
    "    score_last_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPT2 tokenizer\n",
      "Dataloader: padding with token id: 50256\n",
      "Loading data from cache: ('/Users/lukas/Desktop/projects/MIT/data/cache/train-clean-100', 'f0_dct_4.pkl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing samples: 100%|██████████| 31071/31071 [00:17<00:00, 1814.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 1590/31071\n",
      "Loading data from cache: ('/Users/lukas/Desktop/projects/MIT/data/cache/dev-clean', 'f0_dct_4.pkl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing samples: 100%|██████████| 4217/4217 [00:02<00:00, 1917.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 217/4217\n",
      "Loading data from cache: ('/Users/lukas/Desktop/projects/MIT/data/cache/test-clean', 'f0_dct_4.pkl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing samples: 100%|██████████| 4389/4389 [00:02<00:00, 1899.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 270/4389\n",
      "Train dataset size: 29481\n",
      "Validation dataset size: 4000\n",
      "Test dataset size: 4119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of train, val, test in samples: (31071, 4217, 4389)\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = dm.train_texts, dm.train_durations\n",
    "val_texts, val_labels = dm.val_texts, dm.val_durations\n",
    "test_texts, test_labels = dm.test_texts, dm.test_durations\n",
    "\n",
    "print(\n",
    "    f\"Lengths of train, val, test in samples: {len(train_texts), len(val_texts), len(test_texts)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and labels train: (520358, 520358)\n",
      "Words and labels dev: (72537, 72537)\n",
      "Words and labels test: (77979, 77979)\n"
     ]
    }
   ],
   "source": [
    "from src.utils.text_processing import assign_labels_to_sentences\n",
    "\n",
    "all_train_words, all_train_labels = assign_labels_to_sentences(\n",
    "    train_texts, train_labels\n",
    ")\n",
    "all_dev_words, all_dev_labels = assign_labels_to_sentences(val_texts, val_labels)\n",
    "all_test_words, all_test_labels = assign_labels_to_sentences(test_texts, test_labels)\n",
    "\n",
    "print(f\"Words and labels train: {len(all_train_words), len(all_train_labels)}\")\n",
    "print(f\"Words and labels dev: {len(all_dev_words), len(all_dev_labels)}\")\n",
    "print(f\"Words and labels test: {len(all_test_words), len(all_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((520358, 4), (72537, 4), (77979, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_labels = np.array(all_train_labels)\n",
    "all_dev_labels = np.array(all_dev_labels)\n",
    "all_test_labels = np.array(all_test_labels)\n",
    "\n",
    "all_train_labels.shape, all_dev_labels.shape, all_test_labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation and Differential Entropy Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train_size: 52035, n_dev_size: 1450, n_test_size: 23393\n",
      "Finished iteration 1 out of 10 with diff entropy: 9.709436452446255\n",
      "Finished iteration 2 out of 10 with diff entropy: 9.681574546466615\n",
      "Finished iteration 3 out of 10 with diff entropy: 9.671035452211688\n",
      "Finished iteration 4 out of 10 with diff entropy: 9.692982617089081\n",
      "Finished iteration 5 out of 10 with diff entropy: 9.653652964713734\n",
      "Finished iteration 6 out of 10 with diff entropy: 9.673444665170699\n",
      "Finished iteration 7 out of 10 with diff entropy: 9.678973393367961\n",
      "Finished iteration 8 out of 10 with diff entropy: 9.650818798463296\n",
      "Finished iteration 9 out of 10 with diff entropy: 9.684177108634584\n",
      "Finished iteration 10 out of 10 with diff entropy: 9.649133868803366\n",
      "Mean: 9.674522986736728, std: 0.018378846723507404\n"
     ]
    }
   ],
   "source": [
    "# bootstrapping to get confidence intervals\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import gaussian_kde\n",
    "from src.utils.approximation import cross_validate_gkde_bandwidth\n",
    "from src.utils.approximation import monte_carlo_diff_entropy\n",
    "\n",
    "n_iterations = 10\n",
    "n_train_size = int(len(all_train_labels) * 0.1)\n",
    "n_dev_size = int(len(all_dev_labels) * 0.02)\n",
    "n_test_size = int(len(all_test_labels) * 0.3)\n",
    "print(\n",
    "    f\"n_train_size: {n_train_size}, n_dev_size: {n_dev_size}, n_test_size: {n_test_size}\"\n",
    ")\n",
    "\n",
    "diff_entropy_list = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    train_sample = resample(all_train_labels, n_samples=n_train_size)\n",
    "    dev_sample = resample(all_dev_labels, n_samples=n_dev_size)\n",
    "    test_sample = resample(all_test_labels, n_samples=n_test_size)\n",
    "    # best_bw = 0.01\n",
    "    # # best_bw = cross_validate_gkde_bandwidth(train_sample.T, dev_sample.T)\n",
    "    # print(f\"Best bandwidth: {best_bw}\")\n",
    "    density = gaussian_kde(all_train_labels.T, bw_method=0.1)\n",
    "    mc_entropy = monte_carlo_diff_entropy(density, test_sample.T, len(test_sample))\n",
    "    diff_entropy_list.append(mc_entropy)\n",
    "    print(\n",
    "        f\"Finished iteration {i+1} out of {n_iterations} with diff entropy: {mc_entropy}\"\n",
    "    )\n",
    "\n",
    "diff_entropy_list = np.array(diff_entropy_list)\n",
    "print(f\"Mean: {np.mean(diff_entropy_list)}, std: {np.std(diff_entropy_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param scott, score -8.657614158143597\n",
      "new best param scott, score -8.657614158143597\n",
      "param silverman, score -8.76890310133348\n",
      "param 0.01, score -22904.34414780233\n",
      "param 0.1, score -206.66053136976763\n",
      "param 0.3, score -18.74305193892195\n",
      "param 0.5, score -9.18066585609889\n",
      "best bw scott\n"
     ]
    }
   ],
   "source": [
    "from src.utils.approximation import cross_validate_gkde_bandwidth\n",
    "\n",
    "nb_train_samples = 30000\n",
    "nb_test_samples = 5000\n",
    "\n",
    "train_indices = np.random.choice(\n",
    "    np.arange(len(all_train_labels)), nb_train_samples, replace=False\n",
    ")\n",
    "train_data = all_train_labels[train_indices]\n",
    "test_indices = np.random.choice(\n",
    "    np.arange(len(all_test_labels)), nb_test_samples, replace=False\n",
    ")\n",
    "test_data = all_test_labels[test_indices]\n",
    "\n",
    "best_bw = cross_validate_gkde_bandwidth(\n",
    "    train_data=train_data.T,\n",
    "    test_data=test_data.T,\n",
    ")\n",
    "print(f\"best bw {best_bw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel density estimation\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "density = gaussian_kde(np.array(all_train_labels[:20000]).T, bw_method=best_bw)\n",
    "\n",
    "# xs = np.linspace(0, 6, 1000)\n",
    "# plt.plot(xs, density(xs))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differential entropy: 9.0481\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.choice(len(all_test_labels), 50000, replace=False)\n",
    "sampled_labels = all_test_labels[indices]\n",
    "\n",
    "surprisals = density.logpdf(sampled_labels.T)\n",
    "diff_entropy = -np.mean(surprisals)\n",
    "\n",
    "print(f\"Differential entropy: {diff_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Number of label names must match the number of coefficients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# labels_non_nan = [p for ps in train_labels for p in ps if p]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_labels_flat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(train_labels)\n\u001b[0;32m----> 6\u001b[0m plot_vector_kde(\n\u001b[1;32m      7\u001b[0m     train_labels_flat,\n\u001b[1;32m      8\u001b[0m     bw_scalar\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     label_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mDCT-1\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mDCT-2\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mDCT-3\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mDCT-4\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     10\u001b[0m     title\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDistribution of the 4 DCT f0 coefficients\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     \u001b[39m# save_path=\"/Users/lukas/Desktop/projects/MIT/MIT_prosody/precomputed/predictions/energy_mean/energy_distribution.png\",\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/projects/MIT/MIT_prosody/src/utils/plots.py:859\u001b[0m, in \u001b[0;36mplot_vector_kde\u001b[0;34m(data, bw_scalar, label_names, title, save_path)\u001b[0m\n\u001b[1;32m    856\u001b[0m     label_names \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValue \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])]\n\u001b[1;32m    858\u001b[0m \u001b[39m# Check if the number of label names match the number of coefficients\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    860\u001b[0m     \u001b[39mlen\u001b[39m(label_names) \u001b[39m==\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    861\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mNumber of label names must match the number of coefficients\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m \u001b[39m# Plot the KDE for each coefficient\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of label names must match the number of coefficients"
     ]
    }
   ],
   "source": [
    "from src.utils.plots import plot_vector_kde\n",
    "\n",
    "# labels_non_nan = [p for ps in train_labels for p in ps if p]\n",
    "train_labels_flat = np.concatenate(train_labels)\n",
    "\n",
    "plot_vector_kde(\n",
    "    train_labels_flat,\n",
    "    bw_scalar=0.5,\n",
    "    label_names=[\"DCT-1\", \"DCT-2\", \"DCT-3\", \"DCT-4\"],\n",
    "    title=\"Distribution of the 4 DCT f0 coefficients\",\n",
    "    # save_path=\"/Users/lukas/Desktop/projects/MIT/MIT_prosody/precomputed/predictions/energy_mean/energy_distribution.png\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store text and labels as pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"/Users/lukas/Desktop/projects/MIT/data/baseline_data/f0_dct_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(SAVE_PATH + \"/train_words.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_train_words, f)\n",
    "\n",
    "with open(SAVE_PATH + \"/train_labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_train_labels, f)\n",
    "\n",
    "with open(SAVE_PATH + \"/test_words.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_test_words, f)\n",
    "\n",
    "with open(SAVE_PATH + \"/test_labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_test_labels, f)\n",
    "\n",
    "with open(SAVE_PATH + \"/dev_words.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_dev_words, f)\n",
    "\n",
    "with open(SAVE_PATH + \"/dev_labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_dev_labels, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models and Control Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"/Users/lukas/Desktop/projects/MIT/data/baseline_data/f0_dct_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520358 520358\n",
      "72537 72537\n",
      "77979 77979\n"
     ]
    }
   ],
   "source": [
    "# load data again\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open(SAVE_PATH + \"/train_words.pkl\", \"rb\") as f:\n",
    "    all_train_words = pickle.load(f)\n",
    "\n",
    "with open(SAVE_PATH + \"/train_labels.pkl\", \"rb\") as f:\n",
    "    all_train_labels = pickle.load(f)\n",
    "\n",
    "with open(SAVE_PATH + \"/test_words.pkl\", \"rb\") as f:\n",
    "    all_test_words = pickle.load(f)\n",
    "\n",
    "with open(SAVE_PATH + \"/test_labels.pkl\", \"rb\") as f:\n",
    "    all_test_labels = pickle.load(f)\n",
    "\n",
    "with open(SAVE_PATH + \"/dev_words.pkl\", \"rb\") as f:\n",
    "    all_dev_words = pickle.load(f)\n",
    "\n",
    "with open(SAVE_PATH + \"/dev_labels.pkl\", \"rb\") as f:\n",
    "    all_dev_labels = pickle.load(f)\n",
    "\n",
    "print(len(all_train_words), len(all_train_labels))\n",
    "print(len(all_dev_words), len(all_dev_labels))\n",
    "print(len(all_test_words), len(all_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 100%|██████████| 400000/400000 [00:06<00:00, 60027.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.models.baselines.GloVe import GloVeModel\n",
    "\n",
    "glove_model = GloVeModel(\n",
    "    model_path=\"/Users/lukas/Desktop/projects/MIT/data/models/glove/glove.6B.100d.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train, dev, test embeddings: 520358, 72537, 77979\n",
      "Shapes of train, dev, test embeddings: (520358, 100), (72537, 100), (77979, 100)\n",
      "Shapes of train, dev, test labels: (520358, 4), (72537, 4), (77979, 4)\n"
     ]
    }
   ],
   "source": [
    "train_emb = [glove_model.get_word_embedding(word) for word in all_train_words]\n",
    "dev_emb = [glove_model.get_word_embedding(word) for word in all_dev_words]\n",
    "test_emb = [glove_model.get_word_embedding(word) for word in all_test_words]\n",
    "\n",
    "print(\n",
    "    f\"Shapes of train, dev, test embeddings: {len(train_emb)}, {len(dev_emb)}, {len(test_emb)}\"\n",
    ")\n",
    "\n",
    "# create numpy arrays and print shapes\n",
    "import numpy as np\n",
    "\n",
    "train_emb = np.array(train_emb)\n",
    "dev_emb = np.array(dev_emb)\n",
    "test_emb = np.array(test_emb)\n",
    "\n",
    "train_labels = np.array(all_train_labels)\n",
    "dev_labels = np.array(all_dev_labels)\n",
    "test_labels = np.array(all_test_labels)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Shapes of train, dev, test embeddings: {train_emb.shape}, {dev_emb.shape}, {test_emb.shape}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shapes of train, dev, test labels: {train_labels.shape}, {dev_labels.shape}, {test_labels.shape}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn LinReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 2.824094102286586\n",
      "R2 score: 0.05075961504813015\n",
      "Pearson correlation: PearsonRResult(statistic=0.2909611379801414, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "# train sklearn model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(train_emb, all_train_labels)\n",
    "\n",
    "# compute mse\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "predictions = reg.predict(test_emb)\n",
    "mse = mean_absolute_error(all_test_labels, predictions)\n",
    "print(f\"Mean absolute error: {mse}\")\n",
    "\n",
    "# compute r2\n",
    "r2 = r2_score(all_test_labels, predictions)\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "# compute pearson\n",
    "flat_labels = np.concatenate(all_test_labels)\n",
    "flat_predictions = np.concatenate(predictions)\n",
    "pearson = pearsonr(flat_labels, flat_predictions)\n",
    "print(f\"Pearson correlation: {pearson}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
