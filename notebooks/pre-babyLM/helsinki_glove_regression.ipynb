{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prosody.src.data_preparation.helsinki import HelsinkiProminenceExtractor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/Users/lukas/Desktop/projects/MIT/MIT_prosody/data/Helsinki\"\n",
    "save_path = \"/Users/lukas/Desktop/projects/MIT/MIT_prosody/precomputed/predictions\"\n",
    "train_filename = \"train_360.txt\"\n",
    "test_filename = \"test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 116263 utterances\n",
      "Loaded 4822 utterances\n",
      "train utterances 116263, test utterances 4822\n"
     ]
    }
   ],
   "source": [
    "from prosody.src.data_preparation.helsinki import HelsinkiProminenceExtractor\n",
    "\n",
    "train_extractor = HelsinkiProminenceExtractor(root_dir, train_filename)\n",
    "test_extractor = HelsinkiProminenceExtractor(root_dir, test_filename)\n",
    "\n",
    "train_texts = train_extractor.get_all_texts()\n",
    "train_prominences = train_extractor.get_all_real_prominence()\n",
    "\n",
    "test_texts = test_extractor.get_all_texts()\n",
    "test_prominences = test_extractor.get_all_real_prominence()\n",
    "\n",
    "print(f\"train utterances {len(train_texts)}, test utterances {len(test_texts)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GloVe Samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = \"/Users/lukas/Desktop/projects/MIT/MIT_prosody/precomputed/glove\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-11 21:41:21--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-04-11 21:41:21--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-04-11 21:41:22--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘/Users/lukas/Desktop/projects/MIT/MIT_prosody/precomputed/glove/glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822,24M  3,62MB/s    in 8m 52s  \n",
      "\n",
      "2023-04-11 21:50:15 (1,55 MB/s) - ‘/Users/lukas/Desktop/projects/MIT/MIT_prosody/precomputed/glove/glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n",
      "total 1536\n",
      "-rw-r--r--@  1 lukas  staff  659369 Apr 11 21:40 helsinki_stats.ipynb\n",
      "drwxr-xr-x@  9 lukas  staff     288 Apr 11 21:40 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 lukas  staff    6846 Apr 11 21:38 baselines.ipynb\n",
      "-rw-r--r--@  1 lukas  staff   68061 Apr 11 21:22 tokenizer_playground.ipynb\n",
      "-rw-r--r--@  1 lukas  staff   36370 Apr 11 21:19 helsinki_glove_regression.ipynb\n",
      "drwxr-xr-x@ 27 lukas  staff     864 Apr 11 10:50 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 lukas  staff     256 Apr  4 22:42 load_llama.ipynb\n",
      "-rw-r--r--@  1 lukas  staff    4974 Apr  4 22:42 helsinki_dataset.ipynb\n",
      "-rw-r--r--@  1 lukas  staff       0 Apr  3 15:56 .gitkeep\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip -P {glove_dir}\n",
    "!unzip glove.6B.zip\n",
    "!ls -lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dir = (\n",
    "    \"/Users/lukas/Desktop/projects/MIT/prosody/precomputed/models/GloVe_weights\"\n",
    ")\n",
    "\n",
    "vocab, embeddings = [], []\n",
    "with open(os.path.join(weight_dir, \"glove.6B.300d.txt\"), \"rt\") as fi:\n",
    "    full_content = fi.read().strip().split(\"\\n\")\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(\" \")[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(\" \")[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes vocab: (400000,)  embeddings: (400000, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "print(f\"Shapes vocab: {vocab_npa.shape}  embeddings: {embs_npa.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'the' ',' '.' 'of' 'to' 'and' 'in' 'a']\n",
      "(400002, 300)\n"
     ]
    }
   ],
   "source": [
    "vocab_npa = np.insert(vocab_npa, 0, \"<pad>\")\n",
    "vocab_npa = np.insert(vocab_npa, 1, \"<unk>\")\n",
    "print(vocab_npa[:10])\n",
    "\n",
    "pad_emb_npa = np.zeros((1, embs_npa.shape[1]))  # embedding for '<pad>' token.\n",
    "unk_emb_npa = np.mean(embs_npa, axis=0, keepdims=True)  # embedding for '<unk>' token.\n",
    "\n",
    "# insert embeddings for pad and unk tokens at top of embs_npa.\n",
    "embs_npa = np.vstack((pad_emb_npa, unk_emb_npa, embs_npa))\n",
    "print(embs_npa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400002, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "my_embedding_layer = torch.nn.Embedding.from_pretrained(\n",
    "    torch.from_numpy(embs_npa).float()\n",
    ")\n",
    "\n",
    "assert my_embedding_layer.weight.shape == embs_npa.shape\n",
    "print(my_embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab_npa.npy\", \"wb\") as f:\n",
    "    np.save(f, vocab_npa)\n",
    "\n",
    "with open(\"embs_npa.npy\", \"wb\") as f:\n",
    "    np.save(f, embs_npa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {word: i for i, word in enumerate(vocab_npa)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab_npa)}\n",
    "\n",
    "print(word_to_idx[\"house\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"A 'JOLLY' ART CRITIC\", [0.128, 2.454, 0.986, 0.233])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0], prominences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len X: 88534  len y: 88534  failed: 1516\n",
      "failed fraction 0.016835091615769016\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "failed = 0\n",
    "for i, sentence in enumerate(texts):\n",
    "    for j, word in enumerate(sentence.split(\" \")):\n",
    "        try:\n",
    "            word = word.lower()\n",
    "            idx = word_to_idx[word]\n",
    "            label = prominences[i][j]\n",
    "            X.append(torch.tensor([embs_npa[idx]]))\n",
    "            y.append(torch.tensor([label]))\n",
    "        except:\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "print(f\"len X: {len(X)}  len y: {len(y)}  failed: {failed}\")\n",
    "\n",
    "X = torch.cat(X, dim=0)\n",
    "y = torch.cat(y, dim=0)\n",
    "\n",
    "print(f\"failed fraction {failed/(failed+len(X))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of X and y: torch.Size([88534, 300]), torch.Size([88534])\n"
     ]
    }
   ],
   "source": [
    "print(f\"shapes of X and y: {X.shape}, {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the embeddings and labels\n",
    "save_path = \"/Users/lukas/Desktop/projects/MIT/prosody/precomputed/data_embeddings\"\n",
    "data_str = train_filename.replace(\".txt\", \"\")\n",
    "model_name = f\"glove_6B_300d_helsinki_continuous_{data_str}\"\n",
    "torch.save(X, os.path.join(save_path, f\"{model_name}_X.pt\"))\n",
    "torch.save(y, os.path.join(save_path, f\"{model_name}_y.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/Users/lukas/Desktop/projects/MIT/prosody/precomputed/data_embeddings\"\n",
    "model_name = \"glove_6B_300d_helsinki_continuous\"\n",
    "X_train_path = os.path.join(save_path, f\"{model_name}_train_100_X.pt\")\n",
    "y_train_path = os.path.join(save_path, f\"{model_name}_train_100_y.pt\")\n",
    "X_test_path = os.path.join(save_path, f\"{model_name}_test_X.pt\")\n",
    "y_test_path = os.path.join(save_path, f\"{model_name}_test_y.pt\")\n",
    "X_train = torch.load(X_train_path)\n",
    "y_train = torch.load(y_train_path)\n",
    "X_test = torch.load(X_test_path)\n",
    "y_test = torch.load(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train and test set: torch.Size([560915, 300]), torch.Size([88534, 300]), torch.Size([560915]), torch.Size([88534])\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\n",
    "    f\"Shapes of train and test set: {X_train.shape}, {X_test.shape}, {y_train.shape}, {y_test.shape}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prosody.src.models.sklearn.sklearn_models import train_sklearn_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Regressor Scores: {'train': {'mean_absolute_error': 0.6437916, 'mean_squared_error': 0.637565, 'r2_score': 0.0}, 'test': {'mean_absolute_error': 0.6464712, 'mean_squared_error': 0.64568114, 'r2_score': -1.3697985488558828e-05}}\n"
     ]
    }
   ],
   "source": [
    "# Train dummy\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy_regressor = DummyRegressor(strategy=\"mean\")\n",
    "model, scores = train_sklearn_regressor(\n",
    "    dummy_regressor, X_train, y_train, X_test, y_test\n",
    ")\n",
    "print(\"Dummy Regressor Scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression scores: {'train': {'mean_absolute_error': 0.48051304851959553, 'mean_squared_error': 0.44526446002107334, 'r2_score': 0.3016170541255997}, 'test': {'mean_absolute_error': 0.4846849476697734, 'mean_squared_error': 0.4536318040207255, 'r2_score': 0.29742717230591875}}\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regressor = LinearRegression()\n",
    "model, scores = train_sklearn_regressor(\n",
    "    linear_regressor, X_train, y_train, X_test, y_test\n",
    ")\n",
    "print(f\"Linear regression scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist Gradient Boosting Regressor scores: {'train': {'mean_absolute_error': 0.4465646042592211, 'mean_squared_error': 0.3980175811111108, 'r2_score': 0.3757222599957225}, 'test': {'mean_absolute_error': 0.4543714458066604, 'mean_squared_error': 0.4118446241972164, 'r2_score': 0.3621460408458822}}\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hist_regressor = HistGradientBoostingRegressor()\n",
    "model, scores = train_sklearn_regressor(\n",
    "    hist_regressor,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    save_model=True,\n",
    "    save_path=save_path,\n",
    ")\n",
    "print(f\"Hist Gradient Boosting Regressor scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Regressor scores: {'train': {'mean_absolute_error': 0.6449205532514198, 'mean_squared_error': 0.6375879964949015, 'r2_score': -3.621044713986166e-05}, 'test': {'mean_absolute_error': 0.6476010928802634, 'mean_squared_error': 0.6456756848846676, 'r2_score': -5.263480176820323e-06}}\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_regressor = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 10),\n",
    "    max_iter=10,\n",
    "    alpha=1e-4,\n",
    "    solver=\"adam\",\n",
    "    tol=1e-4,\n",
    "    random_state=1,\n",
    "    learning_rate_init=0.1,\n",
    "    early_stopping=True,\n",
    ")\n",
    "model, scores = train_sklearn_regressor(mlp_regressor, X_train, y_train, X_test, y_test)\n",
    "print(f\"MLP Regressor scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fb9be900b071e0a88f0058bf678098d4d0f073e72254ea672133fbf54e537e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
