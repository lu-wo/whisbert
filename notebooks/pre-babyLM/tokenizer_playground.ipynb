{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, AutoModel, GPT2TokenizerFast, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# gpt2_tokenizer_fast = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# gpt2_model = AutoModel.from_pretrained(\"gpt2\")\n",
    "gpt2_large_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apart three paces, hello'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \"I love my dog, because he is brave.\"\n",
    "text = \"Apart three paces, hello\"\n",
    "# text = \"[SOS] \" + text + \" [EOS]\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[39182,  1115,   279,  2114,    11, 23748]]),\n",
       " tensor([[39182,  1115,   279,  2114,    11, 23748]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = gpt2_tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "encoding_large = gpt2_large_tokenizer.encode(\n",
    "    text, add_special_tokens=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "encoding, encoding_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1212,   318,   281,  1672,  6827,    13],\n",
      "        [ 6610,  1672,  6827,    13, 50256, 50256],\n",
      "        [   32,  1790,   530,    13, 50256, 50256]])\n",
      "Attention masks: tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This is an example sentence.', 'Another example sentence.', 'A short one.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the padding token to be the same as the end-of-sequence (EOS) token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create a list of sentences\n",
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "]\n",
    "\n",
    "# Use the tokenizer to encode and pad the sentences\n",
    "encoded_batch = tokenizer.batch_encode_plus(\n",
    "    sentences,\n",
    "    padding=True,  # Enables padding\n",
    "    return_tensors=\"pt\",  # Returns PyTorch tensors (use \"tf\" for TensorFlow tensors)\n",
    ")\n",
    "\n",
    "# Access the padded input IDs and attention masks\n",
    "input_ids = encoded_batch[\"input_ids\"]\n",
    "attention_masks = encoded_batch[\"attention_mask\"]\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention masks:\", attention_masks)\n",
    "\n",
    "# batch decode where attention_mask is used to ignore padding tokens\n",
    "tokenizer.batch_decode(input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# bert tokenizer and fast\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=True)\n",
    "bert_tokenizer_fast = BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-uncased\", do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I must also be thinking of repose , said Meekin ; the journey though most enjoyable has fatigued me .'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \"I love my dog, because he is brave.\"\n",
    "text = \"I must also be thinking of repose , said Meekin ; the journey though most enjoyable has fatigued me .\"\n",
    "# text = \"[SOS] \" + text + \" [EOS]\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i must also be thinking of repose, said meekin ; the journey though most enjoyable has fatigued me. [SEP]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = bert_tokenizer.encode(text, add_special_tokens=True)\n",
    "# encodings_fast = bert_tokenizer_fast.encode(text, add_special_tokens=True)\n",
    "bert_tokenizer.decode(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] The Reverend mr North, in this cool atmosphere, seemed to recover himself, and conversation progressed with some sprightliness. [SEP]'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\n",
    "    101,\n",
    "    1109,\n",
    "    10990,\n",
    "    182,\n",
    "    1197,\n",
    "    1456,\n",
    "    117,\n",
    "    1107,\n",
    "    1142,\n",
    "    4348,\n",
    "    6814,\n",
    "    117,\n",
    "    1882,\n",
    "    1106,\n",
    "    8680,\n",
    "    1471,\n",
    "    117,\n",
    "    1105,\n",
    "    3771,\n",
    "    12687,\n",
    "    1114,\n",
    "    1199,\n",
    "    188,\n",
    "    1643,\n",
    "    12601,\n",
    "    14951,\n",
    "    119,\n",
    "    102,\n",
    "]\n",
    "\n",
    "bert_tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 101, 2023, 2003, 2019, 2742, 6251, 1012,  102],\n",
      "        [ 101, 2178, 2742, 6251, 1012,  102,    0,    0],\n",
      "        [ 101, 1037, 2460, 2028, 1012,  102,    0,    0]])\n",
      "Attention masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "['this is an example sentence.', 'another example sentence.', 'a short one.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create a list of sentences\n",
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "]\n",
    "\n",
    "# Use the tokenizer to encode and pad the sentences\n",
    "encoded_batch = tokenizer.batch_encode_plus(\n",
    "    sentences,\n",
    "    padding=True,  # Enables padding\n",
    "    return_tensors=\"pt\",  # Returns PyTorch tensors (use \"tf\" for TensorFlow tensors)\n",
    ")\n",
    "\n",
    "# Access the padded input IDs and attention masks\n",
    "input_ids = encoded_batch[\"input_ids\"]\n",
    "attention_masks = encoded_batch[\"attention_mask\"]\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention masks:\", attention_masks)\n",
    "\n",
    "# decoding\n",
    "decoding = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf67e5a265b4568a3694be758562d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "]\n",
    "\n",
    "labels = [[1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m encode_and_pad_batch, TokenTaggingDataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[39m=\u001b[39m TokenTaggingDataset(sentences, labels, tokenizer)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.data.components.datasets import encode_and_pad_batch, TokenTaggingDataset\n",
    "\n",
    "dataset = TokenTaggingDataset(sentences, labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: encode_and_pad_batch(batch, tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text \n",
      " This is an example sentence.\n",
      "word encodings \n",
      " [[2023], [2003], [2019], [2742], [6251, 1012]]\n",
      "word_to_token\n",
      " [[0], [1], [2], [3], [4, 5]]\n",
      "#tokens \n",
      " 6\n",
      "algined tokens \n",
      " [2023, 2003, 2019, 2742, 6251, 1012]\n",
      "aligned decoded\n",
      " this is an example sentence.\n",
      "text \n",
      " Another example sentence.\n",
      "word encodings \n",
      " [[2178], [2742], [6251, 1012]]\n",
      "word_to_token\n",
      " [[0], [1], [2, 3]]\n",
      "#tokens \n",
      " 4\n",
      "algined tokens \n",
      " [2178, 2742, 6251, 1012]\n",
      "aligned decoded\n",
      " another example sentence.\n",
      "Input IDs:\n",
      " tensor([[ 101, 2023, 2003, 2019, 2742, 6251, 1012,  102],\n",
      "        [ 101, 2178, 2742, 6251, 1012,  102,    0,    0]])\n",
      "Decoded input:\n",
      " ['[CLS] this is an example sentence. [SEP]', '[CLS] another example sentence. [SEP] [PAD] [PAD]']\n",
      "Attention masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "Padded labels:\n",
      " tensor([[   1,    1,    1,    1,    1,    1, -999, -999],\n",
      "        [   1,    1,    1,    1, -999, -999, -999, -999]])\n",
      "Outputs:\n",
      " tensor([[[-0.3774, -0.3350, -0.3206,  ..., -0.5255,  0.2590,  0.6877],\n",
      "         [-0.8629, -0.6322, -0.4241,  ..., -0.5824,  0.7432,  0.1259],\n",
      "         [-0.2213, -0.9393,  0.3523,  ..., -0.3219,  0.4667,  0.5915],\n",
      "         ...,\n",
      "         [-0.0471, -0.1261,  0.0237,  ..., -0.2642,  0.1113, -0.0666],\n",
      "         [ 0.7917,  0.0632, -0.6609,  ...,  0.3360, -0.7243, -0.2557],\n",
      "         [-0.1416, -0.3367, -0.3314,  ..., -0.2132, -0.4376,  0.4454]],\n",
      "\n",
      "        [[-0.2924, -0.3253, -0.3886,  ..., -0.2868,  0.2157,  0.6433],\n",
      "         [-0.0338, -1.0024, -0.3005,  ..., -0.1190,  0.9047,  0.2311],\n",
      "         [-0.4415, -0.2425, -0.7587,  ..., -0.6208,  0.1528,  0.0825],\n",
      "         ...,\n",
      "         [ 1.0391, -0.0324, -0.4098,  ...,  0.2882, -0.8480, -0.1672],\n",
      "         [-0.3579, -0.4596, -0.1465,  ..., -0.0685,  0.0276,  0.2554],\n",
      "         [-0.3476, -0.5998, -0.2705,  ...,  0.0828,  0.1289,  0.2535]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Outputs shape:\n",
      " torch.Size([2, 8, 768])\n",
      "text \n",
      " A short one.\n",
      "word encodings \n",
      " [[1037], [2460], [2028, 1012]]\n",
      "word_to_token\n",
      " [[0], [1], [2, 3]]\n",
      "#tokens \n",
      " 4\n",
      "algined tokens \n",
      " [1037, 2460, 2028, 1012]\n",
      "aligned decoded\n",
      " a short one.\n",
      "text \n",
      " What a long sentence this here is incredible.\n",
      "word encodings \n",
      " [[2054], [1037], [2146], [6251], [2023], [2182], [2003], [9788, 1012]]\n",
      "word_to_token\n",
      " [[0], [1], [2], [3], [4], [5], [6], [7, 8]]\n",
      "#tokens \n",
      " 9\n",
      "algined tokens \n",
      " [2054, 1037, 2146, 6251, 2023, 2182, 2003, 9788, 1012]\n",
      "aligned decoded\n",
      " what a long sentence this here is incredible.\n",
      "Input IDs:\n",
      " tensor([[ 101, 1037, 2460, 2028, 1012,  102,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2054, 1037, 2146, 6251, 2023, 2182, 2003, 9788, 1012,  102]])\n",
      "Decoded input:\n",
      " ['[CLS] a short one. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] what a long sentence this here is incredible. [SEP]']\n",
      "Attention masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Padded labels:\n",
      " tensor([[   1,    1,    1,    1, -999, -999, -999, -999, -999, -999, -999],\n",
      "        [   1,    1,    1,    1,    1,    1,    1,    1,    1, -999, -999]])\n",
      "Outputs:\n",
      " tensor([[[-0.1449, -0.5376, -0.7223,  ..., -0.1883,  0.2320,  0.4892],\n",
      "         [ 0.4290, -0.1785, -0.9902,  ..., -0.9439,  0.1389,  0.7930],\n",
      "         [ 0.2519,  0.0264, -0.3542,  ..., -0.6387,  0.0040, -0.1435],\n",
      "         ...,\n",
      "         [ 0.1282, -0.2647, -0.0093,  ..., -0.0088,  0.0632,  0.0632],\n",
      "         [-0.2388, -0.8124, -0.1797,  ...,  0.4322,  0.2350, -0.1776],\n",
      "         [-0.1423, -0.8497, -0.1566,  ...,  0.3646,  0.2033, -0.1145]],\n",
      "\n",
      "        [[-0.0040, -0.0363, -0.0509,  ..., -0.1111,  0.2380,  0.5907],\n",
      "         [ 0.6698,  0.7413,  0.1565,  ..., -0.2265,  0.4102,  0.3011],\n",
      "         [ 0.0129,  0.4222,  0.5661,  ...,  0.6035,  0.6153,  0.5633],\n",
      "         ...,\n",
      "         [ 0.6282,  0.7849, -0.0963,  ...,  0.2277,  0.4458,  0.1739],\n",
      "         [ 0.3752,  0.0725, -0.3378,  ...,  0.1718, -0.3127, -0.5080],\n",
      "         [ 0.6674,  0.0567, -0.1778,  ...,  0.0407, -0.5813, -0.4047]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Outputs shape:\n",
      " torch.Size([2, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, BertTokenizer, AutoTokenizer, AutoModel\n",
    "from src.data.components.datasets import encode_and_pad_batch, TokenTaggingDataset\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "\n",
    "# distribute_word_label_to_token function from a previous response\n",
    "\n",
    "# encode_and_pad_batch function from your message\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True, add_prefix_space=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "cfg = DictConfig({\"model\": \"bert-base-uncased\"})\n",
    "\n",
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "    \"What a long sentence this here is incredible.\",\n",
    "]\n",
    "labels = [[1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "dataset = TokenTaggingDataset(sentences, labels, tokenizer, cfg)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=lambda batch: encode_and_pad_batch(batch, tokenizer),\n",
    ")\n",
    "\n",
    "for input_ids, attention_masks, padded_labels in dataloader:\n",
    "    print(\"Input IDs:\\n\", input_ids)\n",
    "    print(\n",
    "        \"Decoded input:\\n\", tokenizer.batch_decode(input_ids, skip_special_tokens=False)\n",
    "    )\n",
    "    print(\"Attention masks:\\n\", attention_masks)\n",
    "    print(\"Padded labels:\\n\", padded_labels)\n",
    "    outputs = model(input_ids, attention_mask=attention_masks).last_hidden_state\n",
    "    print(\"Outputs:\\n\", outputs)\n",
    "    print(\"Outputs shape:\\n\", outputs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.components.datasets import tokenize_text_with_labels\n",
    "from transformers import BertTokenizer, GPT2Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples with different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! This is a test.\"\n",
    "labels = [0, 1, 2, 2, 2, 5]\n",
    "score_first_token = True\n",
    "relative_to_prev = False\n",
    "n_prev = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokenize_text_with_labels() missing 1 required positional argument: 'model_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m, add_prefix_space\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m (\n\u001b[1;32m      6\u001b[0m     input_text,\n\u001b[1;32m      7\u001b[0m     tokenized_text,\n\u001b[1;32m      8\u001b[0m     tokenized_labels,\n\u001b[1;32m      9\u001b[0m     token_ids,\n\u001b[1;32m     10\u001b[0m     mask,\n\u001b[0;32m---> 11\u001b[0m ) \u001b[39m=\u001b[39m tokenize_text_with_labels(\n\u001b[1;32m     12\u001b[0m     text,\n\u001b[1;32m     13\u001b[0m     labels,\n\u001b[1;32m     14\u001b[0m     model_type,\n\u001b[1;32m     15\u001b[0m     score_first_token\u001b[39m=\u001b[39;49mscore_first_token,\n\u001b[1;32m     16\u001b[0m     relative_to_prev\u001b[39m=\u001b[39;49mrelative_to_prev,\n\u001b[1;32m     17\u001b[0m     n_prev\u001b[39m=\u001b[39;49mn_prev,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput text:\u001b[39m\u001b[39m\"\u001b[39m, input_text)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTokenized text:\u001b[39m\u001b[39m\"\u001b[39m, tokenized_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: tokenize_text_with_labels() missing 1 required positional argument: 'model_type'"
     ]
    }
   ],
   "source": [
    "# GPT2\n",
    "model_type = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_prefix_space=True)\n",
    "\n",
    "(\n",
    "    input_text,\n",
    "    tokenized_text,\n",
    "    tokenized_labels,\n",
    "    token_ids,\n",
    "    mask,\n",
    ") = tokenize_text_with_labels(\n",
    "    text,\n",
    "    labels,\n",
    "    model_type,\n",
    "    score_first_token=score_first_token,\n",
    "    relative_to_prev=relative_to_prev,\n",
    "    n_prev=n_prev,\n",
    ")\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Tokenized text:\", tokenized_text)\n",
    "print(\"Tokenized labels:\", tokenized_labels)\n",
    "print(\"Token IDs:\", token_ids)  # decode the token ids\n",
    "print(\"Mask:\", mask)\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokenize_text_with_labels() missing 1 required positional argument: 'model_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbert-cased\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-cased\u001b[39m\u001b[39m\"\u001b[39m, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m (\n\u001b[1;32m      7\u001b[0m     input_text,\n\u001b[1;32m      8\u001b[0m     tokenized_text,\n\u001b[1;32m      9\u001b[0m     tokenized_labels,\n\u001b[1;32m     10\u001b[0m     token_ids,\n\u001b[1;32m     11\u001b[0m     mask,\n\u001b[0;32m---> 12\u001b[0m ) \u001b[39m=\u001b[39m tokenize_text_with_labels(\n\u001b[1;32m     13\u001b[0m     text,\n\u001b[1;32m     14\u001b[0m     labels,\n\u001b[1;32m     15\u001b[0m     model_type,\n\u001b[1;32m     16\u001b[0m     score_first_token\u001b[39m=\u001b[39;49mscore_first_token,\n\u001b[1;32m     17\u001b[0m     relative_to_prev\u001b[39m=\u001b[39;49mrelative_to_prev,\n\u001b[1;32m     18\u001b[0m     n_prev\u001b[39m=\u001b[39;49mn_prev,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput text:\u001b[39m\u001b[39m\"\u001b[39m, input_text)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTokenized text:\u001b[39m\u001b[39m\"\u001b[39m, tokenized_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: tokenize_text_with_labels() missing 1 required positional argument: 'model_type'"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "model_type = \"bert-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", add_special_tokens=True)\n",
    "\n",
    "\n",
    "(\n",
    "    input_text,\n",
    "    tokenized_text,\n",
    "    tokenized_labels,\n",
    "    token_ids,\n",
    "    mask,\n",
    ") = tokenize_text_with_labels(\n",
    "    text,\n",
    "    labels,\n",
    "    model_type,\n",
    "    score_first_token=score_first_token,\n",
    "    relative_to_prev=relative_to_prev,\n",
    "    n_prev=n_prev,\n",
    ")\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Tokenized text:\", tokenized_text)\n",
    "print(\"Tokenized labels:\", tokenized_labels)\n",
    "print(\"Token IDs:\", token_ids)  # decode the token ids\n",
    "print(\"Mask:\", mask)  # decode the token ids\n",
    "tokenizer.decode(token_ids, ignore_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\", add_special_tokens=True\n",
    ")\n",
    "bert_tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.components.helsinki import HelsinkiProminenceExtractor\n",
    "from src.data.components.datasets import TokenTaggingDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = HelsinkiProminenceExtractor(\n",
    "    \"/Users/lukas/Desktop/projects/MIT/prosody/prosody/repositories/helsinki-prosody/data\",\n",
    "    \"train_360.txt\",\n",
    ")\n",
    "texts = extractor.get_all_texts()\n",
    "prominences = extractor.get_all_real_prominence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenTaggingDataset(\n",
    "    texts,\n",
    "    prominences,\n",
    "    bert_tokenizer,\n",
    "    \"bert-cased\",\n",
    "    score_first_token=True,\n",
    "    relative_to_prev=True,\n",
    "    n_prev=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116263"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116263"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.data.components.collators import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = partial(collate_fn, eos_token_id=102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text [\"For man of you your characteristic race Here may he hardy sweet gigantic grow here tower proportionate to Nature Here climb the vast pure spaces unconfined uncheck'd by wall or roof Here laugh with storm or sun here joy here patiently inure Here heed himself unfold himself not others' formulas heed here fill his time To duly fall to aid last To disappear to serve\", \"Tom the Piper's Son\"]\n",
      "tokenized_text [['[CLS]', 'For', 'man', 'of', 'you', 'your', 'characteristic', 'race', 'Here', 'may', 'he', 'hard', '##y', 'sweet', 'gigantic', 'grow', 'here', 'tower', 'proportion', '##ate', 'to', 'Nature', 'Here', 'climb', 'the', 'vast', 'pure', 'spaces', 'un', '##con', '##fine', '##d', 'un', '##che', '##ck', \"'\", 'd', 'by', 'wall', 'or', 'roof', 'Here', 'laugh', 'with', 'storm', 'or', 'sun', 'here', 'joy', 'here', 'patiently', 'in', '##ure', 'Here', 'he', '##ed', 'himself', 'un', '##fold', 'himself', 'not', 'others', \"'\", 'formula', '##s', 'he', '##ed', 'here', 'fill', 'his', 'time', 'To', 'du', '##ly', 'fall', 'to', 'aid', 'last', 'To', 'disappear', 'to', 'serve', '[SEP]'], ['[CLS]', 'Tom', 'the', 'Piper', \"'\", 's', 'Son', '[SEP]']]\n",
      "original_labels [[0.023, 1.304, 0.009, 0.385, 0.112, 0.791, 0.311, 1.903, 0.317, 0.002, 0.821, 0.312, 0.848, 0.407, 2.188, 1.185, 1.412, 0.178, 1.901, 0.872, 3.18, 0.0, 1.324, 1.06, 0.62, 0.381, 2.175, 0.184, 0.852, 0.163, 0.815, 1.01, 0.624, 0.115, 2.086, 0.0, 1.18, 1.164, 3.165, 1.528, 1.003, 1.511, 1.671, 1.237, 0.152, 3.802, 0.062, 1.991, 0.426, 0.301, 0.215, 1.913, 0.422, 0.147, 0.986, 0.059, 1.403, 0.353, 0.226, 1.572, 0.64, 0.123, 1.036, 0.351, 1.174], [2.022, 0.0, 0.65, 0.887]]\n",
      "tokenized_labels tensor([[-1.0000,  0.0230,  1.3040,  0.0090, -0.0603, -0.4540,  0.6223, -0.1183,\n",
      "          1.4983, -0.6847, -0.8417,  0.0803, -1.0000, -0.0680,  0.4697, -0.2533,\n",
      "          1.6657,  0.0373,  0.1520, -1.0000, -1.4170,  0.9760, -0.2917,  2.1963,\n",
      "         -1.9843, -0.0267, -0.4413, -0.1747, -0.6203, -1.0000, -1.0000, -1.0000,\n",
      "          1.4880, -1.0000, -1.0000, -1.0000, -1.0000, -0.8747, -0.0613, -0.9073,\n",
      "          0.4153,  0.4000, -0.0387, -0.7013,  1.5030, -0.9417,  0.4463,  0.0753,\n",
      "          2.3837, -0.3083, -0.9493, -0.3877, -1.0000,  0.3237, -0.1580, -1.0000,\n",
      "         -1.3210,  2.7820, -1.0000, -1.6683,  0.6523, -1.5257, -1.0000, -0.5253,\n",
      "         -1.0000, -0.6910, -1.0000,  1.5990, -0.3877, -0.7030,  0.1587, -0.4593,\n",
      "          1.0057, -1.0000, -0.4630, -0.3790,  0.9113, -0.0770, -0.6897,  0.2577,\n",
      "         -0.2487,  0.6707, -1.0000],\n",
      "        [-1.0000,  2.0220,  0.0000,  0.6500, -1.0000, -1.0000, -0.0037, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000]])\n",
      "input_ids tensor([[  101,  1370,  1299,  1104,  1128,  1240,  7987,  1886,  3446,  1336,\n",
      "          1119,  1662,  1183,  4105, 23275,  4328,  1303,  3590, 10807,  2193,\n",
      "          1106,  7009,  3446,  6767,  1103,  6047,  5805,  6966,  8362,  7235,\n",
      "         24191,  1181,  8362,  4386,  2158,   112,   173,  1118,  2095,  1137,\n",
      "          3664,  3446,  4046,  1114,  4162,  1137,  3336,  1303,  8730,  1303,\n",
      "         21067,  1107,  3313,  3446,  1119,  1174,  1471,  8362, 10787,  1471,\n",
      "          1136,  1639,   112,  7893,  1116,  1119,  1174,  1303,  5475,  1117,\n",
      "          1159,  1706,  3840,  1193,  2303,  1106,  4256,  1314,  1706, 10489,\n",
      "          1106,  2867,   102],\n",
      "        [  101,  2545,  1103, 12558,   112,   188,  6913,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102]])\n",
      "loss_mask tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "word_to_tokens [['For', [1370], 'man', [1299], 'of', [1104], 'you', [1128], 'your', [1240], 'characteristic', [7987], 'race', [1886], 'Here', [3446], 'may', [1336], 'he', [1119], 'hardy', [1662, 1183], 'sweet', [4105], 'gigantic', [23275], 'grow', [4328], 'here', [1303], 'tower', [3590], 'proportionate', [10807, 2193], 'to', [1106], 'Nature', [7009], 'Here', [3446], 'climb', [6767], 'the', [1103], 'vast', [6047], 'pure', [5805], 'spaces', [6966], 'unconfined', [8362, 7235, 24191, 1181], \"uncheck'd\", [8362, 4386, 2158, 112, 173], 'by', [1118], 'wall', [2095], 'or', [1137], 'roof', [3664], 'Here', [3446], 'laugh', [4046], 'with', [1114], 'storm', [4162], 'or', [1137], 'sun', [3336], 'here', [1303], 'joy', [8730], 'here', [1303], 'patiently', [21067], 'inure', [1107, 3313], 'Here', [3446], 'heed', [1119, 1174], 'himself', [1471], 'unfold', [8362, 10787], 'himself', [1471], 'not', [1136], \"others'\", [1639, 112], 'formulas', [7893, 1116], 'heed', [1119, 1174], 'here', [1303], 'fill', [5475], 'his', [1117], 'time', [1159], 'To', [1706], 'duly', [3840, 1193], 'fall', [2303], 'to', [1106], 'aid', [4256], 'last', [1314], 'To', [1706], 'disappear', [10489], 'to', [1106], 'serve', [2867]], ['Tom', [2545], 'the', [1103], \"Piper's\", [12558, 112, 188], 'Son', [6913]]]\n",
      "input_text [\"Tom Tom the piper's son Stole a pig and away he run The pig was eat and Tom was beat And Tom ran crying down the street\", 'There was not a worse vagabond in Shrewsbury than old Barney the piper']\n",
      "tokenized_text [['[CLS]', 'Tom', 'Tom', 'the', 'pipe', '##r', \"'\", 's', 'son', 'St', '##ole', 'a', 'pig', 'and', 'away', 'he', 'run', 'The', 'pig', 'was', 'eat', 'and', 'Tom', 'was', 'beat', 'And', 'Tom', 'ran', 'crying', 'down', 'the', 'street', '[SEP]'], ['[CLS]', 'There', 'was', 'not', 'a', 'worse', 'v', '##aga', '##bon', '##d', 'in', 'Shrewsbury', 'than', 'old', 'Barney', 'the', 'pipe', '##r', '[SEP]']]\n",
      "original_labels [[2.272, 1.773, 0.0, 1.825, 0.533, 1.867, 0.0, 2.261, 0.0, 0.811, 0.134, 0.886, 0.095, 1.954, 0.105, 0.829, 0.211, 1.503, 0.003, 0.976, 0.307, 1.775, 0.0, 1.072, 0.344, 0.176, 0.401], [0.672, 0.112, 2.139, 0.0, 1.386, 1.372, 0.061, 2.372, 0.014, 1.846, 0.639, 0.012, 0.632]]\n",
      "tokenized_labels tensor([[-1.0000,  2.2720,  1.7730,  0.0000,  0.4767, -1.0000, -1.0000, -1.0000,\n",
      "         -0.6663,  1.0810, -1.0000, -1.4083,  1.4610, -1.3760,  0.0573, -0.8900,\n",
      "          0.5710, -0.5153,  1.5823, -0.8733,  0.1110, -0.7517,  1.1213, -0.8447,\n",
      "          0.4037, -0.5203,  1.3463, -1.0193,  0.3780, -0.6050, -0.2960, -0.1297,\n",
      "         -1.0000],\n",
      "        [-1.0000,  0.6720,  0.1120,  2.1390, -0.9743,  0.6357,  0.1970, -1.0000,\n",
      "         -1.0000, -1.0000, -0.8583,  1.4323, -1.2543,  1.0303, -0.7717, -0.8210,\n",
      "         -0.2003, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000]])\n",
      "input_ids tensor([[  101,  2545,  2545,  1103,  9415,  1197,   112,   188,  1488,  1457,\n",
      "          9016,   170, 13407,  1105,  1283,  1119,  1576,  1109, 13407,  1108,\n",
      "          3940,  1105,  2545,  1108,  3222,  1262,  2545,  1868,  6675,  1205,\n",
      "          1103,  2472,   102],\n",
      "        [  101,  1247,  1108,  1136,   170,  4146,   191, 15446,  8868,  1181,\n",
      "          1107, 19561,  1190,  1385, 16213,  1103,  9415,  1197,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102]])\n",
      "loss_mask tensor([[0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "word_to_tokens [['Tom', [2545], 'Tom', [2545], 'the', [1103], \"piper's\", [9415, 1197, 112, 188], 'son', [1488], 'Stole', [1457, 9016], 'a', [170], 'pig', [13407], 'and', [1105], 'away', [1283], 'he', [1119], 'run', [1576], 'The', [1109], 'pig', [13407], 'was', [1108], 'eat', [3940], 'and', [1105], 'Tom', [2545], 'was', [1108], 'beat', [3222], 'And', [1262], 'Tom', [2545], 'ran', [1868], 'crying', [6675], 'down', [1205], 'the', [1103], 'street', [2472]], ['There', [1247], 'was', [1108], 'not', [1136], 'a', [170], 'worse', [4146], 'vagabond', [191, 15446, 8868, 1181], 'in', [1107], 'Shrewsbury', [19561], 'than', [1190], 'old', [1385], 'Barney', [16213], 'the', [1103], 'piper', [9415, 1197]]]\n",
      "input_text ['He never did any work except to play the pipes and he played so badly that few pennies ever found their way into his pouch', \"It was whispered around that old Barney was not very honest but he was so sly and cautious that no one had ever caught him in the act of stealing although a good many things had been missed after they had fallen into the old man's way\"]\n",
      "tokenized_text [['[CLS]', 'He', 'never', 'did', 'any', 'work', 'except', 'to', 'play', 'the', 'pipes', 'and', 'he', 'played', 'so', 'badly', 'that', 'few', 'pen', '##nies', 'ever', 'found', 'their', 'way', 'into', 'his', 'pouch', '[SEP]'], ['[CLS]', 'It', 'was', 'whispered', 'around', 'that', 'old', 'Barney', 'was', 'not', 'very', 'honest', 'but', 'he', 'was', 'so', 'sly', 'and', 'cautious', 'that', 'no', 'one', 'had', 'ever', 'caught', 'him', 'in', 'the', 'act', 'of', 'stealing', 'although', 'a', 'good', 'many', 'things', 'had', 'been', 'missed', 'after', 'they', 'had', 'fallen', 'into', 'the', 'old', 'man', \"'\", 's', 'way', '[SEP]']]\n",
      "original_labels [[0.052, 2.108, 0.007, 0.684, 0.92, 1.466, 0.008, 0.29, 0.0, 1.888, 0.139, 0.044, 1.209, 1.066, 2.382, 0.049, 0.588, 1.138, 1.077, 0.544, 0.0, 0.246, 0.156, 0.044, 1.626], [0.683, 0.378, 1.601, 1.675, 0.201, 1.166, 0.646, 0.012, 1.818, 0.076, 0.788, 0.616, 0.236, 0.125, 3.163, 1.498, 0.166, 1.554, 0.219, 0.111, 0.707, 0.016, 0.208, 1.86, 0.094, 0.129, 0.449, 0.005, 0.097, 2.599, 1.724, 0.0, 1.48, 0.331, 1.615, 0.301, 0.142, 1.72, 1.055, 0.043, 0.025, 1.296, 0.084, 0.032, 0.777, 0.015, 0.524]]\n",
      "tokenized_labels tensor([[-1.0000,  0.0520,  2.1080,  0.0070, -0.0383, -0.0130,  0.9290, -1.0153,\n",
      "         -0.5080, -0.5880,  1.7887, -0.5870, -0.6317,  0.5187,  0.6020,  1.6090,\n",
      "         -1.5033, -0.5777,  0.1317, -1.0000,  0.4853, -0.3903, -0.9197, -0.2943,\n",
      "         -0.1073, -0.0900,  1.4773, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000,  0.6830,  0.3780,  1.6010,  0.7877, -1.0170,  0.0070, -0.3680,\n",
      "         -0.6590,  1.2100, -0.7493,  0.1527, -0.2780, -0.2573, -0.4217,  2.8373,\n",
      "          0.3233, -1.4293, -0.0550, -0.8537, -0.5353,  0.0790, -0.3297, -0.0700,\n",
      "          1.5497, -0.6007, -0.5917, -0.2453, -0.2190, -0.0973,  2.4153,  0.8237,\n",
      "         -1.4733,  0.0390, -0.7370,  1.0113, -0.8410, -0.6070,  1.0340,  0.3340,\n",
      "         -0.9293, -0.9143,  0.9217, -0.3707, -0.4363,  0.3063, -0.2827, -1.0000,\n",
      "         -1.0000,  0.2493, -1.0000]])\n",
      "input_ids tensor([[  101,  1124,  1309,  1225,  1251,  1250,  2589,  1106,  1505,  1103,\n",
      "         13655,  1105,  1119,  1307,  1177,  6118,  1115,  1374,  8228, 16133,\n",
      "          1518,  1276,  1147,  1236,  1154,  1117, 24225,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102],\n",
      "        [  101,  1135,  1108,  3125,  1213,  1115,  1385, 16213,  1108,  1136,\n",
      "          1304,  7345,  1133,  1119,  1108,  1177, 27912,  1105, 18596,  1115,\n",
      "          1185,  1141,  1125,  1518,  2347,  1140,  1107,  1103,  2496,  1104,\n",
      "         11569,  1780,   170,  1363,  1242,  1614,  1125,  1151,  4007,  1170,\n",
      "          1152,  1125,  4984,  1154,  1103,  1385,  1299,   112,   188,  1236,\n",
      "           102]])\n",
      "loss_mask tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 1, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])\n",
      "word_to_tokens [['He', [1124], 'never', [1309], 'did', [1225], 'any', [1251], 'work', [1250], 'except', [2589], 'to', [1106], 'play', [1505], 'the', [1103], 'pipes', [13655], 'and', [1105], 'he', [1119], 'played', [1307], 'so', [1177], 'badly', [6118], 'that', [1115], 'few', [1374], 'pennies', [8228, 16133], 'ever', [1518], 'found', [1276], 'their', [1147], 'way', [1236], 'into', [1154], 'his', [1117], 'pouch', [24225]], ['It', [1135], 'was', [1108], 'whispered', [3125], 'around', [1213], 'that', [1115], 'old', [1385], 'Barney', [16213], 'was', [1108], 'not', [1136], 'very', [1304], 'honest', [7345], 'but', [1133], 'he', [1119], 'was', [1108], 'so', [1177], 'sly', [27912], 'and', [1105], 'cautious', [18596], 'that', [1115], 'no', [1185], 'one', [1141], 'had', [1125], 'ever', [1518], 'caught', [2347], 'him', [1140], 'in', [1107], 'the', [1103], 'act', [2496], 'of', [1104], 'stealing', [11569], 'although', [1780], 'a', [170], 'good', [1363], 'many', [1242], 'things', [1614], 'had', [1125], 'been', [1151], 'missed', [4007], 'after', [1170], 'they', [1152], 'had', [1125], 'fallen', [4984], 'into', [1154], 'the', [1103], 'old', [1385], \"man's\", [1299, 112, 188], 'way', [1236]]]\n",
      "input_text [\"Barney had one son named Tom and they lived all alone in a little hut away at the end of the village street for Tom's mother had died when he was a baby\", \"You may not suppose that Tom was a very good boy since he had such a queer father but neither was he very bad and the worst fault he had was in obeying his father's wishes when Barney wanted him to steal a chicken for their supper or a pot of potatoes for their breakfast\"]\n",
      "tokenized_text [['[CLS]', 'Barney', 'had', 'one', 'son', 'named', 'Tom', 'and', 'they', 'lived', 'all', 'alone', 'in', 'a', 'little', 'hut', 'away', 'at', 'the', 'end', 'of', 'the', 'village', 'street', 'for', 'Tom', \"'\", 's', 'mother', 'had', 'died', 'when', 'he', 'was', 'a', 'baby', '[SEP]'], ['[CLS]', 'You', 'may', 'not', 'suppose', 'that', 'Tom', 'was', 'a', 'very', 'good', 'boy', 'since', 'he', 'had', 'such', 'a', 'que', '##er', 'father', 'but', 'neither', 'was', 'he', 'very', 'bad', 'and', 'the', 'worst', 'fault', 'he', 'had', 'was', 'in', 'obey', '##ing', 'his', 'father', \"'\", 's', 'wishes', 'when', 'Barney', 'wanted', 'him', 'to', 'steal', 'a', 'chicken', 'for', 'their', 'supper', 'or', 'a', 'pot', 'of', 'potatoes', 'for', 'their', 'breakfast', '[SEP]']]\n",
      "original_labels [[1.641, 0.271, 0.27, 2.451, 0.005, 2.647, 0.453, 0.037, 1.86, 0.474, 0.322, 0.003, 0.0, 0.22, 1.084, 1.01, 0.008, 0.024, 0.268, 0.015, 0.112, 0.263, 1.691, 0.407, 2.391, 0.718, 0.223, 1.028, 0.114, 0.101, 0.059, 0.0, 1.166], [0.119, 0.244, 2.899, 1.719, 0.171, 1.173, 0.088, 0.0, 0.161, 1.726, 0.145, 1.459, 0.299, 1.202, 0.173, 0.0, 1.757, 0.96, 0.4, 2.92, 0.04, 0.035, 0.078, 2.285, 0.661, 0.04, 2.403, 0.895, 0.103, 1.035, 0.548, 0.006, 0.491, 0.234, 0.381, 1.983, 0.809, 0.97, 0.422, 0.387, 0.086, 0.589, 0.012, 1.496, 0.139, 0.02, 2.255, 2.034, 0.0, 0.63, 0.015, 0.873, 0.084, 0.03, 0.392]]\n",
      "tokenized_labels tensor([[-1.0000,  1.6410,  0.2710,  0.2700,  1.7237, -0.9923,  1.7383, -1.2480,\n",
      "         -0.9980,  0.8143, -0.3093, -0.4683, -0.8823, -0.2663,  0.1117,  1.0097,\n",
      "          0.5753, -0.7633, -0.6767, -0.0793, -0.0850,  0.0097,  0.1313,  1.5610,\n",
      "         -0.2817,  1.6040, -1.0000, -1.0000, -0.7783, -0.9490, -0.0827, -0.5423,\n",
      "         -0.3540, -0.3553, -0.0913,  1.1127, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000,  0.1190,  0.2440,  2.8990,  0.6317, -1.4497, -0.4233, -0.9330,\n",
      "         -0.4773, -0.2593,  1.6430, -0.4840,  0.7817, -0.8110,  0.5677, -0.8137,\n",
      "         -0.5580,  1.2987, -1.0000,  0.3167, -0.5057,  1.8810, -1.3867, -1.0850,\n",
      "         -0.9203,  2.2340, -0.1383, -0.9680,  1.4077, -0.1397, -1.0097, -0.0987,\n",
      "         -0.1297, -0.5560, -0.0387, -1.0000, -0.1143,  0.1373, -1.0000, -1.0000,\n",
      "          1.6143, -0.0570, -0.0877, -0.8320, -0.3467, -0.5070,  0.2907, -0.3420,\n",
      "          1.2670, -0.5600, -0.5290,  1.7033,  1.2293, -1.4363, -0.7997, -0.8730,\n",
      "          0.6580, -0.4220, -0.2940,  0.0630, -1.0000]])\n",
      "input_ids tensor([[  101, 16213,  1125,  1141,  1488,  1417,  2545,  1105,  1152,  2077,\n",
      "          1155,  2041,  1107,   170,  1376, 16148,  1283,  1120,  1103,  1322,\n",
      "          1104,  1103,  1491,  2472,  1111,  2545,   112,   188,  1534,  1125,\n",
      "          1452,  1165,  1119,  1108,   170,  2963,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102,   102,   102,\n",
      "           102],\n",
      "        [  101,  1192,  1336,  1136,  6699,  1115,  2545,  1108,   170,  1304,\n",
      "          1363,  2298,  1290,  1119,  1125,  1216,   170, 15027,  1200,  1401,\n",
      "          1133,  4534,  1108,  1119,  1304,  2213,  1105,  1103,  4997,  6088,\n",
      "          1119,  1125,  1108,  1107, 17088,  1158,  1117,  1401,   112,   188,\n",
      "          8921,  1165, 16213,  1458,  1140,  1106,  8991,   170,  9323,  1111,\n",
      "          1147, 20644,  1137,   170,  9814,  1104, 15866,  1111,  1147,  6462,\n",
      "           102]])\n",
      "loss_mask tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "word_to_tokens [['Barney', [16213], 'had', [1125], 'one', [1141], 'son', [1488], 'named', [1417], 'Tom', [2545], 'and', [1105], 'they', [1152], 'lived', [2077], 'all', [1155], 'alone', [2041], 'in', [1107], 'a', [170], 'little', [1376], 'hut', [16148], 'away', [1283], 'at', [1120], 'the', [1103], 'end', [1322], 'of', [1104], 'the', [1103], 'village', [1491], 'street', [2472], 'for', [1111], \"Tom's\", [2545, 112, 188], 'mother', [1534], 'had', [1125], 'died', [1452], 'when', [1165], 'he', [1119], 'was', [1108], 'a', [170], 'baby', [2963]], ['You', [1192], 'may', [1336], 'not', [1136], 'suppose', [6699], 'that', [1115], 'Tom', [2545], 'was', [1108], 'a', [170], 'very', [1304], 'good', [1363], 'boy', [2298], 'since', [1290], 'he', [1119], 'had', [1125], 'such', [1216], 'a', [170], 'queer', [15027, 1200], 'father', [1401], 'but', [1133], 'neither', [4534], 'was', [1108], 'he', [1119], 'very', [1304], 'bad', [2213], 'and', [1105], 'the', [1103], 'worst', [4997], 'fault', [6088], 'he', [1119], 'had', [1125], 'was', [1108], 'in', [1107], 'obeying', [17088, 1158], 'his', [1117], \"father's\", [1401, 112, 188], 'wishes', [8921], 'when', [1165], 'Barney', [16213], 'wanted', [1458], 'him', [1140], 'to', [1106], 'steal', [8991], 'a', [170], 'chicken', [9323], 'for', [1111], 'their', [1147], 'supper', [20644], 'or', [1137], 'a', [170], 'pot', [9814], 'of', [1104], 'potatoes', [15866], 'for', [1111], 'their', [1147], 'breakfast', [6462]]]\n",
      "input_text [\"Tom did not like to steal but he had no one to teach him to be honest and so under his father's guidance he fell into bad ways\", 'One morning']\n",
      "tokenized_text [['[CLS]', 'Tom', 'did', 'not', 'like', 'to', 'steal', 'but', 'he', 'had', 'no', 'one', 'to', 'teach', 'him', 'to', 'be', 'honest', 'and', 'so', 'under', 'his', 'father', \"'\", 's', 'guidance', 'he', 'fell', 'into', 'bad', 'ways', '[SEP]'], ['[CLS]', 'One', 'morning', '[SEP]']]\n",
      "original_labels [[2.199, 0.479, 0.335, 1.633, 0.221, 0.411, 0.781, 0.147, 1.306, 0.178, 0.509, 0.0, 0.062, 0.386, 0.025, 0.021, 0.782, 0.46, 2.594, 0.391, 1.215, 0.513, 1.1, 0.226, 1.745, 0.0, 0.972, 0.518], [0.225, 1.81]]\n",
      "tokenized_labels tensor([[-1.0000,  2.1990,  0.4790,  0.3350,  0.6287, -0.5947, -0.3187,  0.0260,\n",
      "         -0.3240,  0.8597, -0.5667, -0.0347, -0.6643, -0.1670,  0.1957, -0.1243,\n",
      "         -0.1367,  0.6380,  0.1840,  2.1730, -0.8877,  0.0667, -0.8870, -1.0000,\n",
      "         -1.0000,  0.3937, -0.7167,  1.1320, -1.0237,  0.3150, -0.3877, -1.0000],\n",
      "        [-1.0000,  0.2250,  1.8100, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]])\n",
      "input_ids tensor([[ 101, 2545, 1225, 1136, 1176, 1106, 8991, 1133, 1119, 1125, 1185, 1141,\n",
      "         1106, 6215, 1140, 1106, 1129, 7345, 1105, 1177, 1223, 1117, 1401,  112,\n",
      "          188, 8815, 1119, 2204, 1154, 2213, 3242,  102],\n",
      "        [ 101, 1448, 2106,  102,  102,  102,  102,  102,  102,  102,  102,  102,\n",
      "          102,  102,  102,  102,  102,  102,  102,  102,  102,  102,  102,  102,\n",
      "          102,  102,  102,  102,  102,  102,  102,  102]])\n",
      "loss_mask tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "word_to_tokens [['Tom', [2545], 'did', [1225], 'not', [1136], 'like', [1176], 'to', [1106], 'steal', [8991], 'but', [1133], 'he', [1119], 'had', [1125], 'no', [1185], 'one', [1141], 'to', [1106], 'teach', [6215], 'him', [1140], 'to', [1106], 'be', [1129], 'honest', [7345], 'and', [1105], 'so', [1177], 'under', [1223], 'his', [1117], \"father's\", [1401, 112, 188], 'guidance', [8815], 'he', [1119], 'fell', [2204], 'into', [1154], 'bad', [2213], 'ways', [3242]], ['One', [1448], 'morning', [2106]]]\n",
      "input_text [\"Tom Tom the piper's son Was hungry when the day begun He wanted a bun and asked for one But soon found out that there were none\", 'What shall we do he asked his father']\n",
      "tokenized_text [['[CLS]', 'Tom', 'Tom', 'the', 'pipe', '##r', \"'\", 's', 'son', 'Was', 'hungry', 'when', 'the', 'day', 'begun', 'He', 'wanted', 'a', 'b', '##un', 'and', 'asked', 'for', 'one', 'But', 'soon', 'found', 'out', 'that', 'there', 'were', 'none', '[SEP]'], ['[CLS]', 'What', 'shall', 'we', 'do', 'he', 'asked', 'his', 'father', '[SEP]']]\n",
      "original_labels [[1.74, 1.233, 0.0, 2.156, 0.571, 0.007, 2.586, 0.068, 0.0, 0.151, 1.648, 0.042, 1.693, 0.005, 1.249, 0.0, 1.51, 0.136, 0.223, 0.876, 2.059, 0.661, 0.928, 0.027, 0.421, 0.192, 0.188], [1.094, 0.363, 0.043, 2.945, 0.002, 0.135, 0.095, 1.091]]\n",
      "tokenized_labels tensor([[-1.0000,  1.7400,  1.2330,  0.0000,  1.1650, -1.0000, -1.0000, -1.0000,\n",
      "         -0.5587, -0.9020,  1.6747, -0.9867, -0.8870, -0.7337,  1.5750, -0.5577,\n",
      "          1.0793, -1.1227,  0.6690, -1.0000, -0.9823,  1.0920, -0.7837, -0.3257,\n",
      "          0.2530,  1.6473, -0.3917, -0.2707, -1.1890, -0.1177, -0.2667, -0.0253,\n",
      "         -1.0000],\n",
      "        [-1.0000,  1.0940,  0.3630,  0.0430,  2.4450, -1.1150, -0.8617, -0.9323,\n",
      "          1.0137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000]])\n",
      "input_ids tensor([[ 101, 2545, 2545, 1103, 9415, 1197,  112,  188, 1488, 3982, 7555, 1165,\n",
      "         1103, 1285, 4972, 1124, 1458,  170,  171, 3488, 1105, 1455, 1111, 1141,\n",
      "         1252, 1770, 1276, 1149, 1115, 1175, 1127, 3839,  102],\n",
      "        [ 101, 1327, 4103, 1195, 1202, 1119, 1455, 1117, 1401,  102,  102,  102,\n",
      "          102,  102,  102,  102,  102,  102,  102,  102,  102,  102,  102,  102,\n",
      "          102,  102,  102,  102,  102,  102,  102,  102,  102]])\n",
      "loss_mask tensor([[0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "word_to_tokens [['Tom', [2545], 'Tom', [2545], 'the', [1103], \"piper's\", [9415, 1197, 112, 188], 'son', [1488], 'Was', [3982], 'hungry', [7555], 'when', [1165], 'the', [1103], 'day', [1285], 'begun', [4972], 'He', [1124], 'wanted', [1458], 'a', [170], 'bun', [171, 3488], 'and', [1105], 'asked', [1455], 'for', [1111], 'one', [1141], 'But', [1252], 'soon', [1770], 'found', [1276], 'out', [1149], 'that', [1115], 'there', [1175], 'were', [1127], 'none', [3839]], ['What', [1327], 'shall', [4103], 'we', [1195], 'do', [1202], 'he', [1119], 'asked', [1455], 'his', [1117], 'father', [1401]]]\n",
      "input_text ['Go hungry replied Barney unless you want to take my pipes and play in the village', 'Perhaps they will give you a penny']\n",
      "tokenized_text [['[CLS]', 'Go', 'hungry', 'replied', 'Barney', 'unless', 'you', 'want', 'to', 'take', 'my', 'pipes', 'and', 'play', 'in', 'the', 'village', '[SEP]'], ['[CLS]', 'Perhaps', 'they', 'will', 'give', 'you', 'a', 'penny', '[SEP]']]\n",
      "original_labels [[0.44, 2.611, 0.185, 0.574, 1.769, 0.0, 0.293, 0.0, 0.67, 0.007, 1.443, 0.853, 0.532, 0.0, 0.003, 1.527], [1.877, 0.2, 0.666, 0.203, 0.131, 0.0, 2.236]]\n",
      "tokenized_labels tensor([[-1.0000,  0.4400,  2.6110,  0.1850, -0.5047,  0.6457, -0.8427, -0.4880,\n",
      "         -0.6873,  0.5723, -0.3140,  1.2173,  0.1463, -0.2357, -0.9427, -0.4587,\n",
      "          1.3487, -1.0000],\n",
      "        [-1.0000,  1.8770,  0.2000,  0.6660, -0.7113, -0.2253, -0.3333,  2.1247,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000]])\n",
      "input_ids tensor([[  101,  3414,  7555,  3028, 16213,  4895,  1128,  1328,  1106,  1321,\n",
      "          1139, 13655,  1105,  1505,  1107,  1103,  1491,   102],\n",
      "        [  101,  5203,  1152,  1209,  1660,  1128,   170, 24585,   102,   102,\n",
      "           102,   102,   102,   102,   102,   102,   102,   102]])\n",
      "loss_mask tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "word_to_tokens [['Go', [3414], 'hungry', [7555], 'replied', [3028], 'Barney', [16213], 'unless', [4895], 'you', [1128], 'want', [1328], 'to', [1106], 'take', [1321], 'my', [1139], 'pipes', [13655], 'and', [1105], 'play', [1505], 'in', [1107], 'the', [1103], 'village', [1491]], ['Perhaps', [5203], 'they', [1152], 'will', [1209], 'give', [1660], 'you', [1128], 'a', [170], 'penny', [24585]]]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    for k, v in batch.items():\n",
    "        print(k, v)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\", add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = bert_tokenizer.decode(\n",
    "    batch[\"input_ids\"][0].tolist(), skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A'JOLLY'ART CRITIC\""
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/deberta-v2-xxlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:720\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    721\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1811\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1811\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1812\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1813\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1814\u001b[0m     init_configuration,\n\u001b[1;32m   1815\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1816\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1817\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1818\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1819\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1820\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1821\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1965\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1963\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1965\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1966\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1967\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1968\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1969\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1970\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:133\u001b[0m, in \u001b[0;36mDebertaV2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    120\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    132\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    134\u001b[0m         vocab_file,\n\u001b[1;32m    135\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    136\u001b[0m         do_lower_case\u001b[39m=\u001b[39;49mdo_lower_case,\n\u001b[1;32m    137\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[1;32m    138\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    139\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    140\u001b[0m         sep_token\u001b[39m=\u001b[39;49msep_token,\n\u001b[1;32m    141\u001b[0m         pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    142\u001b[0m         cls_token\u001b[39m=\u001b[39;49mcls_token,\n\u001b[1;32m    143\u001b[0m         mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    144\u001b[0m         split_by_punct\u001b[39m=\u001b[39;49msplit_by_punct,\n\u001b[1;32m    145\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_by_punct \u001b[39m=\u001b[39m split_by_punct\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m TokenizerFast\u001b[39m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1288\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn instance of tokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_name\u001b[39m}\u001b[39;00m\u001b[39m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1283\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1284\u001b[0m     )\n\u001b[1;32m   1286\u001b[0m converter_class \u001b[39m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m-> 1288\u001b[0m \u001b[39mreturn\u001b[39;00m converter_class(transformer_tokenizer)\u001b[39m.\u001b[39mconverted()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:445\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    441\u001b[0m requires_backends(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprotobuf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    443\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 445\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m sentencepiece_model_pb2 \u001b[39mas\u001b[39;00m model_pb2\n\u001b[1;32m    447\u001b[0m m \u001b[39m=\u001b[39m model_pb2\u001b[39m.\u001b[39mModelProto()\n\u001b[1;32m    448\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_tokenizer\u001b[39m.\u001b[39mvocab_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/transformers/utils/sentencepiece_model_pb2.py:91\u001b[0m\n\u001b[1;32m     25\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m     28\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     29\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece_model.proto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     package\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     ),\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     84\u001b[0m _TRAINERSPEC_MODELTYPE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     85\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece.TrainerSpec.ModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m     file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     89\u001b[0m     create_key\u001b[39m=\u001b[39m_descriptor\u001b[39m.\u001b[39m_internal_create_key,\n\u001b[1;32m     90\u001b[0m     values\u001b[39m=\u001b[39m[\n\u001b[0;32m---> 91\u001b[0m         _descriptor\u001b[39m.\u001b[39;49mEnumValueDescriptor(\n\u001b[1;32m     92\u001b[0m             name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mUNIGRAM\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     93\u001b[0m             index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     94\u001b[0m             number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     95\u001b[0m             serialized_options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     96\u001b[0m             \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     97\u001b[0m             create_key\u001b[39m=\u001b[39;49m_descriptor\u001b[39m.\u001b[39;49m_internal_create_key,\n\u001b[1;32m     98\u001b[0m         ),\n\u001b[1;32m     99\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    100\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBPE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    101\u001b[0m             index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    102\u001b[0m             number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    103\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m             create_key\u001b[39m=\u001b[39m_descriptor\u001b[39m.\u001b[39m_internal_create_key,\n\u001b[1;32m    106\u001b[0m         ),\n\u001b[1;32m    107\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    108\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWORD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m             index\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    110\u001b[0m             number\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    111\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    113\u001b[0m             create_key\u001b[39m=\u001b[39m_descriptor\u001b[39m.\u001b[39m_internal_create_key,\n\u001b[1;32m    114\u001b[0m         ),\n\u001b[1;32m    115\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    116\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCHAR\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    117\u001b[0m             index\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    118\u001b[0m             number\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m    119\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m             create_key\u001b[39m=\u001b[39m_descriptor\u001b[39m.\u001b[39m_internal_create_key,\n\u001b[1;32m    122\u001b[0m         ),\n\u001b[1;32m    123\u001b[0m     ],\n\u001b[1;32m    124\u001b[0m     containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m     serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m     serialized_start\u001b[39m=\u001b[39m\u001b[39m1294\u001b[39m,\n\u001b[1;32m    127\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m1347\u001b[39m,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m _sym_db\u001b[39m.\u001b[39mRegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\n\u001b[1;32m    131\u001b[0m _MODELPROTO_SENTENCEPIECE_TYPE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mEnumDescriptor(\n\u001b[1;32m    132\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece.ModelProto.SentencePiece.Type\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m2184\u001b[39m,\n\u001b[1;32m    191\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/google/protobuf/descriptor.py:796\u001b[0m, in \u001b[0;36mEnumValueDescriptor.__new__\u001b[0;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, index, number,\n\u001b[1;32m    794\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m    795\u001b[0m             options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 796\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[1;32m    797\u001b[0m   \u001b[39m# There is no way we can build a complete EnumValueDescriptor with the\u001b[39;00m\n\u001b[1;32m    798\u001b[0m   \u001b[39m# given parameters (the name of the Enum is not known, for example).\u001b[39;00m\n\u001b[1;32m    799\u001b[0m   \u001b[39m# Fortunately generated files just pass it to the EnumDescriptor()\u001b[39;00m\n\u001b[1;32m    800\u001b[0m   \u001b[39m# constructor, which will ignore it, so returning None is good enough.\u001b[39;00m\n\u001b[1;32m    801\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Hello', ',', '▁world', '!', '</s>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.tokenize(\"Hello, world!\", add_special_tokens=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
