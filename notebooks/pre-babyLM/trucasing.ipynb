{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# packages needed\n",
    "# !pip install nltk\n",
    "# !pip install stanfordnlp\n",
    "# !pip install --upgrade bleu\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import stanfordnlp\n",
    "from bleu import list_bleu\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(truecased_words, original_words, verbose=True):\n",
    "    correct = sum(\n",
    "        1 for orig, true in zip(original_words, truecased_words) if orig == true\n",
    "    )\n",
    "    accuracy = correct / len(original_words)\n",
    "    if verbose:\n",
    "        print(f\"Accuracy of casing: {accuracy}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/lukas/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/lukas/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/lukas/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Vector file is not provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# init packages\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# nltk.download(\"punkt\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# nltk.download(\"averaged_perceptron_tagger\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# stanfordnlp.download(\"en\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m stf_nlp \u001b[39m=\u001b[39m stanfordnlp\u001b[39m.\u001b[39;49mPipeline(processors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenize,mwt,pos\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# function for restoring capitalization\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtruecasing\u001b[39m(input_text):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/stanfordnlp/pipeline/core.py:121\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, processors, lang, models_dir, treebank, use_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mprint\u001b[39m(curr_processor_config)\n\u001b[1;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name] \u001b[39m=\u001b[39m NAME_TO_PROCESSOR_CLASS[processor_name](config\u001b[39m=\u001b[39;49mcurr_processor_config,\n\u001b[1;32m    122\u001b[0m                                                                               pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    123\u001b[0m                                                                               use_gpu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_gpu)\n\u001b[1;32m    124\u001b[0m \u001b[39mexcept\u001b[39;00m ProcessorRequirementsException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    125\u001b[0m     \u001b[39m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     pipeline_reqs_exceptions\u001b[39m.\u001b[39mappend(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/stanfordnlp/pipeline/processor.py:102\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_up_model(config, use_gpu)\n\u001b[1;32m    103\u001b[0m \u001b[39m# run set up process\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m# build the final config for the processor\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_up_final_config(config)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/stanfordnlp/pipeline/pos_processor.py:24\u001b[0m, in \u001b[0;36mPOSProcessor._set_up_model\u001b[0;34m(self, config, use_gpu)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pretrain \u001b[39m=\u001b[39m Pretrain(config[\u001b[39m'\u001b[39m\u001b[39mpretrain_path\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[39m# set up trainer\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39m=\u001b[39m Trainer(pretrain\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrain, model_file\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mmodel_path\u001b[39;49m\u001b[39m'\u001b[39;49m], use_cuda\u001b[39m=\u001b[39;49muse_gpu)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/stanfordnlp/models/pos/trainer.py:32\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, vocab, pretrain, model_file, use_cuda)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_cuda \u001b[39m=\u001b[39m use_cuda\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m model_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[39m# load everything from file\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload(pretrain, model_file)\n\u001b[1;32m     33\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(var \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m [args, vocab, pretrain])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/stanfordnlp/models/pos/trainer.py:107\u001b[0m, in \u001b[0;36mTrainer.load\u001b[0;34m(self, pretrain, filename)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m MultiVocab\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Tagger(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab, emb_matrix\u001b[39m=\u001b[39mpretrain\u001b[39m.\u001b[39;49memb, share_hid\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mshare_hid\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m], strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/stanfordnlp/models/common/pretrain.py:33\u001b[0m, in \u001b[0;36mPretrain.emb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39memb\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_emb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_emb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/stanfordnlp/models/common/pretrain.py:46\u001b[0m, in \u001b[0;36mPretrain.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m data[\u001b[39m'\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39memb\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_and_save()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/prosody/lib/python3.9/site-packages/stanfordnlp/models/common/pretrain.py:51\u001b[0m, in \u001b[0;36mPretrain.read_and_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_and_save\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     49\u001b[0m     \u001b[39m# load from pretrained filename\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vec_filename \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mVector file is not provided.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mReading pretrained vectors from \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vec_filename))\n\u001b[1;32m     54\u001b[0m     \u001b[39m# first try reading as xz file, if failed retry as text file\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Vector file is not provided."
     ]
    }
   ],
   "source": [
    "# init packages\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# stanfordnlp.download(\"en\")\n",
    "stf_nlp = stanfordnlp.Pipeline(processors=\"tokenize,mwt,pos\")\n",
    "\n",
    "\n",
    "# function for restoring capitalization\n",
    "def truecasing(input_text):\n",
    "    \"\"\"\n",
    "    Achieves 86.71% as a measure of the quality of truecasing repair on YELP Kagggle dataset\n",
    "    \"\"\"\n",
    "    # split the text into sentences\n",
    "    sentences = sent_tokenize(input_text, language=\"english\")\n",
    "    # capitalize the sentences\n",
    "    sentences_capitalized = [s.capitalize() for s in sentences]\n",
    "    # join the capitalized sentences\n",
    "    text_truecase = re.sub(\" (?=[\\.,'!?:;])\", \"\", \" \".join(sentences_capitalized))\n",
    "    # capitalize words according to part-of-speech tagging (POS)\n",
    "    doc = stf_nlp(text_truecase)\n",
    "    text_truecase = \" \".join(\n",
    "        [\n",
    "            w.text.capitalize() if w.upos in [\"PROPN\", \"NNS\"] else w.text\n",
    "            for sent in doc.sentences\n",
    "            for w in sent.words\n",
    "        ]\n",
    "    )\n",
    "    text_truecase = re.sub(r'\\s([?.!\"](?:\\s|$))', r\"\\1\", text_truecase)\n",
    "    # Remove extra spaces before punctuation marks\n",
    "    text_truecase = re.sub(r'\\s+([?.!-,:;\"])', r\"\\1\", text_truecase)\n",
    "    return text_truecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the banking sector tumult stems from the Fed’s rapid interest rate increases over the past year. Central bankers are expected to lift rates to just above 5 percent this week, up from near-zero as recently as March 2022. After that quick series of adjustments, many lenders are facing losses on older securities and loans, which pay relatively low interest rates compared with newer securities issued in a higher-rate world.\n",
      "Some of the banking sector tumult stems from the fed ’s rapid interest rate increases over the past year. Central bankers are expected to lift rates to just above 5 percent this week, up from near - zero as recently as March 2022. After that quick series of adjustments, many lenders are facing losses on older securities and loans, which pay relatively low interest rates compared with newer securities issued in a higher - rate world.\n"
     ]
    }
   ],
   "source": [
    "text = \"Some of the banking sector tumult stems from the Fed’s rapid interest rate increases over the past year. Central bankers are expected to lift rates to just above 5 percent this week, up from near-zero as recently as March 2022. After that quick series of adjustments, many lenders are facing losses on older securities and loans, which pay relatively low interest rates compared with newer securities issued in a higher-rate world.\"\n",
    "lower = text.lower()\n",
    "\n",
    "pred = truecasing(text)\n",
    "\n",
    "print(text)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import truecase\n",
    "\n",
    "\n",
    "def truecase_truecase(input_text):\n",
    "    return truecase.get_true_case(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think that John stone is a nice guy . There is a stone on the grass . I'm fat . are you welcome and smart in London? is this Martin's dog?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truecase_truecase(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Powell could offer some signal during his news conference, or he could opt to leave the Fed ’ s options open — which is what some economists expect.\n",
      "Accuracy of casing: 0.6296296296296297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6296296296296297"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Mr. Powell could offer some signal during his news conference, or he could opt to leave the Fed’s options open — which is what some economists expect.\"\n",
    "lower = text.lower()\n",
    "pred = truecase_truecase(lower)\n",
    "print(pred)\n",
    "\n",
    "test_accuracy(pred.split(), text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text truecase is Mr. Powell could offer some signal during his news conference , or he could opt to leave the fed ’s options open — which is what some economists expect .\n",
      "after sub is Mr. Powell could offer some signal during his news conference , or he could opt to leave the fed ’s options open — which is what some economists expect.\n",
      "Accuracy of casing: 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = truecasing(lower)\n",
    "test_accuracy(pred2.split(), text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think that john stone is a nice guy. There is a stone on the grass. I'm fat. Are you welcome and smart in london? Is this martin's dog?\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "\n",
    "def truecasing_by_sentence_segmentation(input_text):\n",
    "    # split the text into sentences\n",
    "    sentences = sent_tokenize(input_text, language=\"english\")\n",
    "    # capitalize the sentences\n",
    "    sentences_capitalized = [s.capitalize() for s in sentences]\n",
    "    # join the capitalized sentences\n",
    "    text_truecase = re.sub(\" (?=[\\.,'!?:;])\", \"\", \" \".join(sentences_capitalized))\n",
    "    return text_truecase\n",
    "\n",
    "\n",
    "truecasing_by_sentence_segmentation(text)\n",
    "\"I think that john stone is a nice guy. There is a stone on the grass. I'm fat. Are you welcome and smart in london? Is this martin's dog?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Powell could offer some Signal during his News Conference, or he could opt to leave the Fed ’ S Options open — which is what some Economists expect.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "\n",
    "def truecasing_by_pos(input_text):\n",
    "    # tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # apply POS-tagging on words\n",
    "    tagged_words = nltk.pos_tag([word.lower() for word in words])\n",
    "    # apply capitalization based on POS tags\n",
    "    capitalized_words = [\n",
    "        w.capitalize() if t in [\"NN\", \"NNS\"] else w for (w, t) in tagged_words\n",
    "    ]\n",
    "    # capitalize first word in sentence\n",
    "    capitalized_words[0] = capitalized_words[0].capitalize()\n",
    "    # join capitalized words\n",
    "    text_truecase = re.sub(\" (?=[\\.,'!?:;])\", \"\", \" \".join(capitalized_words))\n",
    "    return text_truecase\n",
    "\n",
    "\n",
    "truecasing_by_pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
